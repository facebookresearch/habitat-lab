VERBOSE: False
BASE_TASK_CONFIG_PATH: configs/tasks/rearrange/nav_pick.yaml
TASK_CONFIG:
  TASK:
    REWARD_MEASURE: "composite_reward"
    SUCCESS_MEASURE: "composite_success"
    SUCCESS_REWARD: 100.0
    SLACK_REWARD: -0.01
    USING_SUBTASKS: True
    SENSORS:
      - TARGET_START_SENSOR
      - GOAL_SENSOR
      - JOINT_SENSOR
      - IS_HOLDING_SENSOR
      - END_EFFECTOR_SENSOR
      - RELATIVE_RESTING_POS_SENSOR
      - TARGET_START_POINT_GOAL_SENSOR
      - NAV_TO_SKILL_SENSOR
      - MARKER_REL_POS_SENSOR
      - ART_JOINT_SENSOR
      # Needed to replace the TARGET_START_POINT_GOAL_SENSOR
      - TARGET_START_GPS_COMPASS_SENSOR
      - TARGET_GOAL_GPS_COMPASS_SENSOR
    MEASUREMENTS:
      - OBJECT_TO_GOAL_DISTANCE
      - ROBOT_FORCE
      - FORCE_TERMINATE
      - ROBOT_COLLS
      - END_EFFECTOR_TO_REST_DISTANCE
      - END_EFFECTOR_TO_OBJECT_DISTANCE
      - OBJ_AT_GOAL
      - ROT_DIST_TO_GOAL
      - DIST_TO_GOAL
      - BAD_CALLED_TERMINATE
      - NAV_TO_POS_SUCC
      - REARRANGE_NAV_TO_OBJ_SUCCESS
      - REARRANGE_NAV_TO_OBJ_REWARD
      - PICK_SUCCESS
      - PICK_REWARD
      - PLACE_SUCCESS
      - PLACE_REWARD
      - COMPOSITE_NODE_IDX
      - DOES_WANT_TERMINATE
      - COMPOSITE_SUCCESS
      - COMPOSITE_BAD_CALLED_TERMINATE
      - COMPOSITE_REWARD
      - NUM_STEPS
      - DID_VIOLATE_HOLD_CONSTRAINT
      - EE_DIST_TO_MARKER
      - ART_OBJ_AT_DESIRED_STATE
      - ART_OBJ_STATE
      - ART_OBJ_SUCCESS
      - ART_OBJ_REWARD
      - MOVE_OBJECTS_REWARD
TRAINER_NAME: "ddppo"
ENV_NAME: "RearrangeRLEnv"
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: ["disk"]
TENSORBOARD_DIR: "tb"
VIDEO_DIR: "video_dir"
VIDEO_FPS: 30
VIDEO_RENDER_TOP_DOWN: False
VIDEO_RENDER_ALL_INFO: True
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: "data/new_checkpoints"
NUM_ENVIRONMENTS: 1
WRITER_TYPE: 'tb'
# Visual sensors to include
SENSORS: ["HEAD_DEPTH_SENSOR"]
CHECKPOINT_FOLDER: "data/new_checkpoints"
NUM_UPDATES: -1
TOTAL_NUM_STEPS: 1.0e8
LOG_INTERVAL: 10
NUM_CHECKPOINTS: 20
FORCE_TORCH_SINGLE_THREADED: True
EVAL_KEYS_TO_INCLUDE_IN_NAME: ['composite_reward', 'force', 'composite_success']
EVAL:
  SPLIT: "val"
  USE_CKPT_CONFIG: False
  SHOULD_LOAD_CKPT: False

RL:
  POLICY:
      name: "HierarchicalPolicy"
      high_level_policy:
        name: "GtHighLevelPolicy"
      DEFINED_SKILLS:
        NN_OPEN_CAB:
          skill_name: "ArtObjSkillPolicy"
          name: "PointNavResNetPolicy"
          action_distribution_type: "gaussian"
          AT_RESTING_THRESHOLD: 0.15
          OBS_SKILL_INPUTS: []
          LOAD_CKPT_FILE: "data/models/open_cab.pth"
          MAX_SKILL_STEPS: 200
          START_ZONE_RADIUS: 0.3
          FORCE_END_ON_TIMEOUT: True

        NN_CLOSE_CAB:
          skill_name: "ArtObjSkillPolicy"
          name: "PointNavResNetPolicy"
          action_distribution_type: "gaussian"
          AT_RESTING_THRESHOLD: 0.15
          OBS_SKILL_INPUTS: []
          LOAD_CKPT_FILE: "data/models/close_cab.pth"
          MAX_SKILL_STEPS: 200
          START_ZONE_RADIUS: 0.3
          FORCE_END_ON_TIMEOUT: True

        NN_OPEN_FRIDGE:
          skill_name: "ArtObjSkillPolicy"
          name: "PointNavResNetPolicy"
          action_distribution_type: "gaussian"
          AT_RESTING_THRESHOLD: 0.15
          OBS_SKILL_INPUTS: []
          LOAD_CKPT_FILE: "data/models/open_fridge.pth"
          MAX_SKILL_STEPS: 200
          START_ZONE_RADIUS: 0.3
          FORCE_END_ON_TIMEOUT: True

        NN_CLOSE_FRIDGE:
          skill_name: "ArtObjSkillPolicy"
          name: "PointNavResNetPolicy"
          action_distribution_type: "gaussian"
          AT_RESTING_THRESHOLD: 0.15
          OBS_SKILL_INPUTS: []
          LOAD_CKPT_FILE: "data/models/close_fridge.pth"
          MAX_SKILL_STEPS: 200
          START_ZONE_RADIUS: 0.3
          FORCE_END_ON_TIMEOUT: True

        NN_PICK:
          skill_name: "PickSkillPolicy"
          name: "PointNavResNetPolicy"
          action_distribution_type: "gaussian"
          AT_RESTING_THRESHOLD: 0.15
          OBS_SKILL_INPUTS: ["obj_start_sensor"]
          LOAD_CKPT_FILE: "data/models/pick.pth"
          MAX_SKILL_STEPS: 200
          FORCE_END_ON_TIMEOUT: True

        NN_PLACE:
          skill_name: "PlaceSkillPolicy"
          name: "PointNavResNetPolicy"
          action_distribution_type: "gaussian"
          AT_RESTING_THRESHOLD: 0.15
          OBS_SKILL_INPUTS: ["obj_goal_sensor"]
          LOAD_CKPT_FILE: "data/models/place.pth"
          MAX_SKILL_STEPS: 200
          FORCE_END_ON_TIMEOUT: True

        NN_NAV:
          skill_name: "NavSkillPolicy"
          name: "PointNavResNetPolicy"
          action_distribution_type: "gaussian"
          OBS_SKILL_INPUTS: ["object_to_agent_gps_compass"]
          OBS_SKILL_INPUT_DIM: 2
          LIN_SPEED_STOP: 0.067
          ANG_SPEED_STOP: 0.067
          LOAD_CKPT_FILE: "data/models/nav.pth"
          MAX_SKILL_STEPS: 300
          FORCE_END_ON_TIMEOUT: False

        WAIT_SKILL:
          skill_name: "WaitSkillPolicy"
          MAX_SKILL_STEPS: -1.0
          FORCE_END_ON_TIMEOUT: False

        RESET_ARM_SKILL:
          skill_name: "ResetArmSkill"
          MAX_SKILL_STEPS: 50
          RESET_JOINT_STATE: [-4.5003259e-01, -1.0799699e00, 9.9526465e-02, 9.3869519e-01, -7.8854430e-04, 1.5702540e00, 4.6168058e-03]
          FORCE_END_ON_TIMEOUT: False

      USE_SKILLS:
        open_cab: "NN_OPEN_CAB"
        close_cab: "NN_CLOSE_CAB"
        open_fridge: "NN_OPEN_FRIDGE"
        close_fridge: "NN_CLOSE_FRIDGE"
        pick: "NN_PICK"
        place: "NN_PLACE"
        nav: "NN_NAV"
        nav_to_receptacle: "NN_NAV"
        reset_arm: "RESET_ARM_SKILL"



  PPO:
    # ppo params
    clip_param: 0.2
    ppo_epoch: 2
    num_mini_batch: 2
    value_loss_coef: 0.5
    entropy_coef: 0.0001
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 128
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 50

    use_normalized_advantage: False

    hidden_size: 512

    # Use double buffered sampling, typically helps
    # when environment time is similar or large than
    # policy inference time during rollout generation
    use_double_buffered_sampler: False

  DDPPO:
    sync_frac: 0.6
    # The PyTorch distributed backend to use
    distrib_backend: NCCL
    # Visual encoder backbone
    pretrained_weights: data/ddppo-models/gibson-2plus-resnet50.pth
    # Initialize with pretrained weights
    pretrained: False
    # Initialize just the visual encoder backbone with pretrained weights
    pretrained_encoder: False
    # Whether or not the visual encoder backbone will be trained.
    train_encoder: True
    # Whether or not to reset the critic linear layer
    reset_critic: False

    # Model parameters
    backbone: resnet50
    rnn_type: LSTM
    num_recurrent_layers: 2
