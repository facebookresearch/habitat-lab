{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Furniture Rearrangement - How to setup a new interaction task in Habitat-Lab\n",
    "\n",
    "This tutorial demonstrates how to setup a new task in Habitat that utilizes interaction capabilities in Habitat Simulator.\n",
    "\n",
    "![teaser](https://drive.google.com/uc?id=1pupGvb4dGefd0T_23GpeDkkcIocDHSL_)\n",
    "\n",
    "## Task Definition:\n",
    "The working example in this demo will be the task of **Furniture Rearrangement** - The agent will be randomly spawned in an environment in which the furniture are initially displaced from their desired position. The agent is tasked with navigating the environment, picking furniture and putting them in the desired position. To keep the tutorial simple and easy to follow, we will rearrange just a single object.\n",
    "\n",
    "To setup this task, we will build on top of existing API in Habitat-Simulator and Habitat-Lab. Here is a summary of all the steps involved in setting up this task:\n",
    "\n",
    "1. **Setup the Simulator**: Using existing functionalities of the Habitat-Sim, we can add or remove objects from the scene. We will use these methods to spawn the agent and the objects at some pre-defined initial configuration.\n",
    "2. **Create a New Dataset**: We will define a new dataset class to save / load a list of episodes for the agent to train and evaluate on.\n",
    "3. **Grab / Release Action**: We will add the \"grab/release\" action to the agent's action space to allow the agent to pickup / drop an object under a crosshair.\n",
    "4. **Extend the Simulator Class**: We will extend the Simulator Class to add support for new actions implemented in previous step and add other additional utility functions\n",
    "5. **Create a New Task**: Create a new task definition, implement new *sensors* and *metrics*.\n",
    "6. **Train an RL agent**: We will define rewards for this task and utilize it to train an RL agent using the PPO algorithm.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Installation { display-mode: \"form\" }\n",
    "# @markdown (double click to show code).\n",
    "\n",
    "!curl -L https://raw.githubusercontent.com/facebookresearch/habitat-sim/master/examples/colab_utils/colab_install.sh | NIGHTLY=true bash -s\n",
    "%cd /content\n",
    "\n",
    "!gdown --id 1Pc-J6pZzXEd8RSeLM94t3iwO8q_RQ853\n",
    "!unzip -o /content/coda.zip -d /content/habitat-sim/data/scene_datasets\n",
    "\n",
    "# reload the cffi version\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import importlib\n",
    "\n",
    "    import cffi\n",
    "\n",
    "    importlib.reload(cffi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Path Setup and Imports { display-mode: \"form\" }\n",
    "# @markdown (double click to show code).\n",
    "\n",
    "%cd /content/habitat-lab\n",
    "\n",
    "## [setup]\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List, Optional, Type\n",
    "\n",
    "import attr\n",
    "import cv2\n",
    "import git\n",
    "import magnum as mn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import habitat\n",
    "import habitat_sim\n",
    "from habitat.config import Config\n",
    "from habitat.core.registry import registry\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/usr/bin/ffmpeg\"\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "%cd $dir_path\n",
    "data_path = os.path.join(dir_path, \"data\")\n",
    "output_directory = \"data/tutorials/output/\"  # @param {type:\"string\"}\n",
    "output_path = os.path.join(dir_path, output_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-display\", dest=\"display\", action=\"store_false\")\n",
    "    parser.add_argument(\n",
    "        \"--no-make-video\", dest=\"make_video\", action=\"store_false\"\n",
    "    )\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.display\n",
    "    display = args.display\n",
    "    make_video = args.make_video\n",
    "else:\n",
    "    show_video = False\n",
    "    make_video = False\n",
    "    display = False\n",
    "\n",
    "if make_video and not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Util functions to visualize observations\n",
    "# @markdown - `make_video_cv2`: Renders a video from a list of observations\n",
    "# @markdown - `simulate`: Runs simulation for a given amount of time at 60Hz\n",
    "# @markdown - `simulate_and_make_vid` Runs simulation and creates video\n",
    "\n",
    "\n",
    "def make_video_cv2(\n",
    "    observations, cross_hair=None, prefix=\"\", open_vid=True, fps=60\n",
    "):\n",
    "    sensor_keys = list(observations[0])\n",
    "    videodims = observations[0][sensor_keys[0]].shape\n",
    "    videodims = (videodims[1], videodims[0])  # flip to w,h order\n",
    "    print(videodims)\n",
    "    video_file = output_path + prefix + \".mp4\"\n",
    "    print(\"Encoding the video: %s \" % video_file)\n",
    "    writer = vut.get_fast_video_writer(video_file, fps=fps)\n",
    "    for ob in observations:\n",
    "        # If in RGB/RGBA format, remove the alpha channel\n",
    "        rgb_im_1st_person = cv2.cvtColor(ob[\"rgb\"], cv2.COLOR_RGBA2RGB)\n",
    "        if cross_hair is not None:\n",
    "            rgb_im_1st_person[\n",
    "                cross_hair[0] - 2 : cross_hair[0] + 2,\n",
    "                cross_hair[1] - 2 : cross_hair[1] + 2,\n",
    "            ] = [255, 0, 0]\n",
    "\n",
    "        if rgb_im_1st_person.shape[:2] != videodims:\n",
    "            rgb_im_1st_person = cv2.resize(\n",
    "                rgb_im_1st_person, videodims, interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "        # write the 1st person observation to video\n",
    "        writer.append_data(rgb_im_1st_person)\n",
    "    writer.close()\n",
    "\n",
    "    if open_vid:\n",
    "        print(\"Displaying video\")\n",
    "        vut.display_video(video_file)\n",
    "\n",
    "\n",
    "def simulate(sim, dt=1.0, get_frames=True):\n",
    "    # simulate dt seconds at 60Hz to the nearest fixed timestep\n",
    "    print(\"Simulating \" + str(dt) + \" world seconds.\")\n",
    "    observations = []\n",
    "    start_time = sim.get_world_time()\n",
    "    while sim.get_world_time() < start_time + dt:\n",
    "        sim.step_physics(1.0 / 60.0)\n",
    "        if get_frames:\n",
    "            observations.append(sim.get_sensor_observations())\n",
    "    return observations\n",
    "\n",
    "\n",
    "# convenience wrapper for simulate and make_video_cv2\n",
    "def simulate_and_make_vid(sim, crosshair, prefix, dt=1.0, open_vid=True):\n",
    "    observations = simulate(sim, dt)\n",
    "    make_video_cv2(observations, crosshair, prefix=prefix, open_vid=open_vid)\n",
    "\n",
    "\n",
    "def display_sample(\n",
    "    rgb_obs,\n",
    "    semantic_obs=np.array([]),\n",
    "    depth_obs=np.array([]),\n",
    "    key_points=None,  # noqa: B006\n",
    "):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGB\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\n",
    "            \"P\", (semantic_obs.shape[1], semantic_obs.shape[0])\n",
    "        )\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray(\n",
    "            (depth_obs / 10 * 255).astype(np.uint8), mode=\"L\"\n",
    "        )\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        # plot points on images\n",
    "        if key_points is not None:\n",
    "            for point in key_points:\n",
    "                plt.plot(\n",
    "                    point[0], point[1], marker=\"o\", markersize=10, alpha=0.8\n",
    "                )\n",
    "        plt.imshow(data)\n",
    "\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the Simulator\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Setup simulator configuration\n",
    "# @markdown We'll start with setting up simulator with the following configurations\n",
    "# @markdown - The simulator will render both RGB, Depth observations of 256x256 resolution.\n",
    "# @markdown - The actions available will be `move_forward`, `turn_left`, `turn_right`.\n",
    "\n",
    "\n",
    "def make_cfg(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.default_agent_id = settings[\"default_agent_id\"]\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "    sim_cfg.enable_physics = settings[\"enable_physics\"]\n",
    "    sim_cfg.physics_config_file = settings[\"physics_config_file\"]\n",
    "\n",
    "    # Note: all sensors must have the same resolution\n",
    "    sensor_specs = []\n",
    "\n",
    "    rgb_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    rgb_sensor_spec.uuid = \"rgb\"\n",
    "    rgb_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    rgb_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    rgb_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    rgb_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(rgb_sensor_spec)\n",
    "\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = \"depth\"\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    depth_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(depth_sensor_spec)\n",
    "\n",
    "    # Here you can specify the amount of displacement in a forward action and the turn angle\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\": habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=0.1)\n",
    "        ),\n",
    "        \"turn_left\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=10.0)\n",
    "        ),\n",
    "        \"turn_right\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=10.0)\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n",
    "\n",
    "\n",
    "settings = {\n",
    "    \"max_frames\": 10,\n",
    "    \"width\": 256,\n",
    "    \"height\": 256,\n",
    "    \"scene\": \"data/scene_datasets/coda/coda.glb\",\n",
    "    \"default_agent_id\": 0,\n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters\n",
    "    \"rgb\": True,  # RGB sensor\n",
    "    \"depth\": True,  # Depth sensor\n",
    "    \"seed\": 1,\n",
    "    \"enable_physics\": True,\n",
    "    \"physics_config_file\": \"data/default.physics_config.json\",\n",
    "    \"silent\": False,\n",
    "    \"compute_shortest_path\": False,\n",
    "    \"compute_action_shortest_path\": False,\n",
    "    \"save_png\": True,\n",
    "}\n",
    "\n",
    "cfg = make_cfg(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Spawn the agent at a pre-defined location\n",
    "\n",
    "\n",
    "def init_agent(sim):\n",
    "    agent_pos = np.array([-0.15776923, 0.18244143, 0.2988735])\n",
    "\n",
    "    # Place the agent\n",
    "    sim.agents[0].scene_node.translation = agent_pos\n",
    "    agent_orientation_y = -40\n",
    "    sim.agents[0].scene_node.rotation = mn.Quaternion.rotation(\n",
    "        mn.Deg(agent_orientation_y), mn.Vector3(0, 1.0, 0)\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.sim_cfg.default_agent_id = 0\n",
    "with habitat_sim.Simulator(cfg) as sim:\n",
    "    init_agent(sim)\n",
    "    if make_video:\n",
    "        # Visualize the agent's initial position\n",
    "        simulate_and_make_vid(\n",
    "            sim, None, \"sim-init\", dt=1.0, open_vid=show_video\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Set the object's initial and final position\n",
    "# @markdown Defines two utility functions:\n",
    "# @markdown - `remove_all_objects`: This will remove all objects from the scene\n",
    "# @markdown - `set_object_in_front_of_agent`: This will add an object in the scene in front of the agent at the specified distance.\n",
    "\n",
    "# @markdown Here we add a chair *3.0m* away from the agent and the task is to place the agent at the desired final position which is *7.0m* in front of the agent.\n",
    "\n",
    "\n",
    "def remove_all_objects(sim):\n",
    "    for obj_id in sim.get_existing_object_ids():\n",
    "        sim.remove_object(obj_id)\n",
    "\n",
    "\n",
    "def set_object_in_front_of_agent(sim, obj_id, z_offset=-1.5):\n",
    "    r\"\"\"\n",
    "    Adds an object in front of the agent at some distance.\n",
    "    \"\"\"\n",
    "    agent_transform = sim.agents[0].scene_node.transformation_matrix()\n",
    "    obj_translation = agent_transform.transform_point(\n",
    "        np.array([0, 0, z_offset])\n",
    "    )\n",
    "    sim.set_translation(obj_translation, obj_id)\n",
    "\n",
    "    obj_node = sim.get_object_scene_node(obj_id)\n",
    "    xform_bb = habitat_sim.geo.get_transformed_bb(\n",
    "        obj_node.cumulative_bb, obj_node.transformation\n",
    "    )\n",
    "\n",
    "    # also account for collision margin of the scene\n",
    "    scene_collision_margin = 0.04\n",
    "    y_translation = mn.Vector3(\n",
    "        0, xform_bb.size_y() / 2.0 + scene_collision_margin, 0\n",
    "    )\n",
    "    sim.set_translation(y_translation + sim.get_translation(obj_id), obj_id)\n",
    "\n",
    "\n",
    "def init_objects(sim):\n",
    "    # Manager of Object Attributes Templates\n",
    "    obj_attr_mgr = sim.get_object_template_manager()\n",
    "    obj_attr_mgr.load_configs(\n",
    "        str(os.path.join(data_path, \"test_assets/objects\"))\n",
    "    )\n",
    "\n",
    "    # Add a chair into the scene.\n",
    "    obj_path = \"test_assets/objects/chair\"\n",
    "    chair_template_id = obj_attr_mgr.load_object_configs(\n",
    "        str(os.path.join(data_path, obj_path))\n",
    "    )[0]\n",
    "    chair_attr = obj_attr_mgr.get_template_by_ID(chair_template_id)\n",
    "    obj_attr_mgr.register_template(chair_attr)\n",
    "\n",
    "    # Object's initial position 3m away from the agent.\n",
    "    object_id = sim.add_object_by_handle(chair_attr.handle)\n",
    "    set_object_in_front_of_agent(sim, object_id, -3.0)\n",
    "    sim.set_object_motion_type(\n",
    "        habitat_sim.physics.MotionType.STATIC, object_id\n",
    "    )\n",
    "\n",
    "    # Object's final position 7m away from the agent\n",
    "    goal_id = sim.add_object_by_handle(chair_attr.handle)\n",
    "    set_object_in_front_of_agent(sim, goal_id, -7.0)\n",
    "    sim.set_object_motion_type(habitat_sim.physics.MotionType.STATIC, goal_id)\n",
    "\n",
    "    return object_id, goal_id\n",
    "\n",
    "\n",
    "with habitat_sim.Simulator(cfg) as sim:\n",
    "    init_agent(sim)\n",
    "    init_objects(sim)\n",
    "\n",
    "    # Visualize the scene after the chair is added into the scene.\n",
    "    if make_video:\n",
    "        simulate_and_make_vid(\n",
    "            sim, None, \"object-init\", dt=1.0, open_vid=show_video\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearrangement Dataset\n",
    "![dataset](https://drive.google.com/uc?id=1y0qS0MifmJsZ0F4jsRZGI9BrXzslFLn7)\n",
    "\n",
    "In the previous section, we created a single episode of the rearrangement task. Let's define a format to store all the necessary information about a single episode. It should store the *scene* the episode belongs to, *initial spawn position and orientation* of the agent, *object type*, object's *initial position and orientation* as well as *final position and orientation*.\n",
    "\n",
    "The format will be as follows:\n",
    "```\n",
    "{\n",
    "  'episode_id': 0,\n",
    "  'scene_id': 'data/scene_datasets/coda/coda.glb',\n",
    "  'goals': {\n",
    "    'position': [4.34, 0.67, -5.06],\n",
    "    'rotation': [0.0, 0.0, 0.0, 1.0]\n",
    "   },\n",
    "  'objects': {\n",
    "    'object_id': 0,\n",
    "    'object_template': 'data/test_assets/objects/chair',\n",
    "    'position': [1.77, 0.67, -1.99],\n",
    "    'rotation': [0.0, 0.0, 0.0, 1.0]\n",
    "  },\n",
    "  'start_position': [-0.15, 0.18, 0.29],\n",
    "  'start_rotation': [-0.0, -0.34, -0.0, 0.93]}\n",
    "}\n",
    "```\n",
    "Once an episode is defined, a dataset will just be a collection of such episodes. For simplicity, in this notebook, the dataset will only contain one episode defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Create a new dataset\n",
    "# @markdown Utility functions to define and save the dataset for the rearrangement task\n",
    "\n",
    "\n",
    "def get_rotation(sim, object_id):\n",
    "    quat = sim.get_rotation(object_id)\n",
    "    return np.array(quat.vector).tolist() + [quat.scalar]\n",
    "\n",
    "\n",
    "def init_episode_dict(episode_id, scene_id, agent_pos, agent_rot):\n",
    "    episode_dict = {\n",
    "        \"episode_id\": episode_id,\n",
    "        \"scene_id\": \"data/scene_datasets/coda/coda.glb\",\n",
    "        \"start_position\": agent_pos,\n",
    "        \"start_rotation\": agent_rot,\n",
    "        \"info\": {},\n",
    "    }\n",
    "    return episode_dict\n",
    "\n",
    "\n",
    "def add_object_details(sim, episode_dict, obj_id, object_template, object_id):\n",
    "    object_template = {\n",
    "        \"object_id\": obj_id,\n",
    "        \"object_template\": object_template,\n",
    "        \"position\": np.array(sim.get_translation(object_id)).tolist(),\n",
    "        \"rotation\": get_rotation(sim, object_id),\n",
    "    }\n",
    "    episode_dict[\"objects\"] = object_template\n",
    "    return episode_dict\n",
    "\n",
    "\n",
    "def add_goal_details(sim, episode_dict, object_id):\n",
    "    goal_template = {\n",
    "        \"position\": np.array(sim.get_translation(object_id)).tolist(),\n",
    "        \"rotation\": get_rotation(sim, object_id),\n",
    "    }\n",
    "    episode_dict[\"goals\"] = goal_template\n",
    "    return episode_dict\n",
    "\n",
    "\n",
    "# set the number of objects to 1 always for now.\n",
    "def build_episode(sim, episode_num, object_id, goal_id):\n",
    "    episodes = {\"episodes\": []}\n",
    "    for episode in range(episode_num):\n",
    "        agent_state = sim.get_agent(0).get_state()\n",
    "        agent_pos = np.array(agent_state.position).tolist()\n",
    "        agent_quat = agent_state.rotation\n",
    "        agent_rot = np.array(agent_quat.vec).tolist() + [agent_quat.real]\n",
    "        episode_dict = init_episode_dict(\n",
    "            episode, settings[\"scene\"], agent_pos, agent_rot\n",
    "        )\n",
    "\n",
    "        object_attr = sim.get_object_initialization_template(object_id)\n",
    "        object_path = os.path.relpath(\n",
    "            os.path.splitext(object_attr.render_asset_handle)[0]\n",
    "        )\n",
    "\n",
    "        episode_dict = add_object_details(\n",
    "            sim, episode_dict, 0, object_path, object_id\n",
    "        )\n",
    "        episode_dict = add_goal_details(sim, episode_dict, goal_id)\n",
    "        episodes[\"episodes\"].append(episode_dict)\n",
    "\n",
    "    return episodes\n",
    "\n",
    "\n",
    "with habitat_sim.Simulator(cfg) as sim:\n",
    "    init_agent(sim)\n",
    "    object_id, goal_id = init_objects(sim)\n",
    "\n",
    "    episodes = build_episode(sim, 1, object_id, goal_id)\n",
    "\n",
    "    dataset_content_path = \"data/datasets/rearrangement/coda/v1/train/\"\n",
    "    if not os.path.exists(dataset_content_path):\n",
    "        os.makedirs(dataset_content_path)\n",
    "\n",
    "    with gzip.open(\n",
    "        os.path.join(dataset_content_path, \"train.json.gz\"), \"wt\"\n",
    "    ) as f:\n",
    "        json.dump(episodes, f)\n",
    "\n",
    "    print(\n",
    "        \"Dataset written to {}\".format(\n",
    "            os.path.join(dataset_content_path, \"train.json.gz\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dataset class to read the saved dataset in Habitat-Lab.\n",
    "# @markdown To read the saved episodes in Habitat-Lab, we will extend the `Dataset` class and the `Episode` base class. It will help provide all the relevant details about the episode through a consistent API to all downstream tasks.\n",
    "\n",
    "# @markdown - We will first create a `RearrangementEpisode` by extending the `NavigationEpisode` to include additional information about object's initial configuration and desired final configuration.\n",
    "# @markdown - We will then define a `RearrangementDatasetV0` class that builds on top of `PointNavDatasetV1` class to read the JSON file stored earlier and initialize a list of `RearrangementEpisode`.\n",
    "\n",
    "from habitat.core.utils import DatasetFloatJSONEncoder, not_none_validator\n",
    "from habitat.datasets.pointnav.pointnav_dataset import (\n",
    "    CONTENT_SCENES_PATH_FIELD,\n",
    "    DEFAULT_SCENE_PATH_PREFIX,\n",
    "    PointNavDatasetV1,\n",
    ")\n",
    "from habitat.tasks.nav.nav import NavigationEpisode\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True, kw_only=True)\n",
    "class RearrangementSpec:\n",
    "    r\"\"\"Specifications that capture a particular position of final position\n",
    "    or initial position of the object.\n",
    "    \"\"\"\n",
    "\n",
    "    position: List[float] = attr.ib(default=None, validator=not_none_validator)\n",
    "    rotation: List[float] = attr.ib(default=None, validator=not_none_validator)\n",
    "    info: Optional[Dict[str, str]] = attr.ib(default=None)\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True, kw_only=True)\n",
    "class RearrangementObjectSpec(RearrangementSpec):\n",
    "    r\"\"\"Object specifications that capture position of each object in the scene,\n",
    "    the associated object template.\n",
    "    \"\"\"\n",
    "    object_id: str = attr.ib(default=None, validator=not_none_validator)\n",
    "    object_template: Optional[str] = attr.ib(\n",
    "        default=\"data/test_assets/objects/chair\"\n",
    "    )\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True, kw_only=True)\n",
    "class RearrangementEpisode(NavigationEpisode):\n",
    "    r\"\"\"Specification of episode that includes initial position and rotation\n",
    "    of agent, all goal specifications, all object specifications\n",
    "\n",
    "    Args:\n",
    "        episode_id: id of episode in the dataset\n",
    "        scene_id: id of scene inside the simulator.\n",
    "        start_position: numpy ndarray containing 3 entries for (x, y, z).\n",
    "        start_rotation: numpy ndarray with 4 entries for (x, y, z, w)\n",
    "            elements of unit quaternion (versor) representing agent 3D\n",
    "            orientation.\n",
    "        goal: object's goal position and rotation\n",
    "        object: object's start specification defined with object type,\n",
    "            position, and rotation.\n",
    "    \"\"\"\n",
    "    objects: RearrangementObjectSpec = attr.ib(\n",
    "        default=None, validator=not_none_validator\n",
    "    )\n",
    "    goals: RearrangementSpec = attr.ib(\n",
    "        default=None, validator=not_none_validator\n",
    "    )\n",
    "\n",
    "\n",
    "@registry.register_dataset(name=\"RearrangementDataset-v0\")\n",
    "class RearrangementDatasetV0(PointNavDatasetV1):\n",
    "    r\"\"\"Class inherited from PointNavDataset that loads Rearrangement dataset.\"\"\"\n",
    "    episodes: List[RearrangementEpisode]\n",
    "    content_scenes_path: str = \"{data_path}/content/{scene}.json.gz\"\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        result = DatasetFloatJSONEncoder().encode(self)\n",
    "        return result\n",
    "\n",
    "    def __init__(self, config: Optional[Config] = None) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "    def from_json(\n",
    "        self, json_str: str, scenes_dir: Optional[str] = None\n",
    "    ) -> None:\n",
    "        deserialized = json.loads(json_str)\n",
    "        if CONTENT_SCENES_PATH_FIELD in deserialized:\n",
    "            self.content_scenes_path = deserialized[CONTENT_SCENES_PATH_FIELD]\n",
    "\n",
    "        for i, episode in enumerate(deserialized[\"episodes\"]):\n",
    "            rearrangement_episode = RearrangementEpisode(**episode)\n",
    "            rearrangement_episode.episode_id = str(i)\n",
    "\n",
    "            if scenes_dir is not None:\n",
    "                if rearrangement_episode.scene_id.startswith(\n",
    "                    DEFAULT_SCENE_PATH_PREFIX\n",
    "                ):\n",
    "                    rearrangement_episode.scene_id = (\n",
    "                        rearrangement_episode.scene_id[\n",
    "                            len(DEFAULT_SCENE_PATH_PREFIX) :\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                rearrangement_episode.scene_id = os.path.join(\n",
    "                    scenes_dir, rearrangement_episode.scene_id\n",
    "                )\n",
    "\n",
    "            rearrangement_episode.objects = RearrangementObjectSpec(\n",
    "                **rearrangement_episode.objects\n",
    "            )\n",
    "            rearrangement_episode.goals = RearrangementSpec(\n",
    "                **rearrangement_episode.goals\n",
    "            )\n",
    "\n",
    "            self.episodes.append(rearrangement_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Load the saved dataset using the Dataset class\n",
    "config = habitat.get_config(\"configs/datasets/pointnav/habitat_test.yaml\")\n",
    "config.defrost()\n",
    "config.DATASET.DATA_PATH = (\n",
    "    \"data/datasets/rearrangement/coda/v1/{split}/{split}.json.gz\"\n",
    ")\n",
    "config.DATASET.TYPE = \"RearrangementDataset-v0\"\n",
    "config.freeze()\n",
    "\n",
    "dataset = RearrangementDatasetV0(config.DATASET)\n",
    "\n",
    "# check if the dataset got correctly deserialized\n",
    "assert len(dataset.episodes) == 1\n",
    "\n",
    "assert dataset.episodes[0].objects.position == [\n",
    "    1.770593523979187,\n",
    "    0.6726829409599304,\n",
    "    -1.9992598295211792,\n",
    "]\n",
    "assert dataset.episodes[0].objects.rotation == [0.0, 0.0, 0.0, 1.0]\n",
    "assert (\n",
    "    dataset.episodes[0].objects.object_template\n",
    "    == \"data/test_assets/objects/chair\"\n",
    ")\n",
    "\n",
    "assert dataset.episodes[0].goals.position == [\n",
    "    4.3417439460754395,\n",
    "    0.6726829409599304,\n",
    "    -5.0634379386901855,\n",
    "]\n",
    "assert dataset.episodes[0].goals.rotation == [0.0, 0.0, 0.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Grab/Release Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title RayCast utility to implement Grab/Release Under Cross-Hair Action\n",
    "# @markdown Cast a ray in the direction of crosshair from the camera and check if it collides with another object within a certain distance threshold\n",
    "\n",
    "\n",
    "def raycast(sim, sensor_name, crosshair_pos=(128, 128), max_distance=2.0):\n",
    "    r\"\"\"Cast a ray in the direction of crosshair and check if it collides\n",
    "    with another object within a certain distance threshold\n",
    "    :param sim: Simulator object\n",
    "    :param sensor_name: name of the visual sensor to be used for raycasting\n",
    "    :param crosshair_pos: 2D coordiante in the viewport towards which the\n",
    "        ray will be cast\n",
    "    :param max_distance: distance threshold beyond which objects won't\n",
    "        be considered\n",
    "    \"\"\"\n",
    "    render_camera = sim._sensors[sensor_name]._sensor_object.render_camera\n",
    "    center_ray = render_camera.unproject(mn.Vector2i(crosshair_pos))\n",
    "\n",
    "    raycast_results = sim.cast_ray(center_ray, max_distance=max_distance)\n",
    "\n",
    "    closest_object = -1\n",
    "    closest_dist = 1000.0\n",
    "    if raycast_results.has_hits():\n",
    "        for hit in raycast_results.hits:\n",
    "            if hit.ray_distance < closest_dist:\n",
    "                closest_dist = hit.ray_distance\n",
    "                closest_object = hit.object_id\n",
    "\n",
    "    return closest_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Test the raycast utility.\n",
    "\n",
    "with habitat_sim.Simulator(cfg) as sim:\n",
    "    init_agent(sim)\n",
    "    obj_attr_mgr = sim.get_object_template_manager()\n",
    "    obj_attr_mgr.load_configs(\n",
    "        str(os.path.join(data_path, \"test_assets/objects\"))\n",
    "    )\n",
    "    obj_path = \"test_assets/objects/chair\"\n",
    "    chair_template_id = obj_attr_mgr.load_object_configs(\n",
    "        str(os.path.join(data_path, obj_path))\n",
    "    )[0]\n",
    "    chair_attr = obj_attr_mgr.get_template_by_ID(chair_template_id)\n",
    "    obj_attr_mgr.register_template(chair_attr)\n",
    "    object_id = sim.add_object_by_handle(chair_attr.handle)\n",
    "    print(f\"Chair's object id is {object_id}\")\n",
    "\n",
    "    set_object_in_front_of_agent(sim, object_id, -1.5)\n",
    "    sim.set_object_motion_type(\n",
    "        habitat_sim.physics.MotionType.STATIC, object_id\n",
    "    )\n",
    "    if make_video:\n",
    "        # Visualize the agent's initial position\n",
    "        simulate_and_make_vid(\n",
    "            sim, [190, 128], \"sim-before-grab\", dt=1.0, open_vid=show_video\n",
    "        )\n",
    "\n",
    "    # Distance threshold=2 is greater than agent-to-chair distance.\n",
    "    # Should return chair's object id\n",
    "    closest_object = raycast(\n",
    "        sim, \"rgb\", crosshair_pos=[128, 190], max_distance=2.0\n",
    "    )\n",
    "    print(f\"Closest Object ID: {closest_object} using 2.0 threshold\")\n",
    "    assert (\n",
    "        closest_object == object_id\n",
    "    ), f\"Could not pick chair with ID: {object_id}\"\n",
    "\n",
    "    # Distance threshold=1 is smaller than agent-to-chair distance .\n",
    "    # Should return -1\n",
    "    closest_object = raycast(\n",
    "        sim, \"rgb\", crosshair_pos=[128, 190], max_distance=1.0\n",
    "    )\n",
    "    print(f\"Closest Object ID: {closest_object} using 1.0 threshold\")\n",
    "    assert closest_object == -1, \"Agent shoud not be able to pick any object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define a Grab/Release action and create a new action space.\n",
    "# @markdown Each new action is defined by a `ActionSpec` and an `ActuationSpec`. `ActionSpec` is mapping between the action name and its corresponding `ActuationSpec`. `ActuationSpec` contains all the necessary specifications required to define the action.\n",
    "\n",
    "from habitat.config.default import _C, CN\n",
    "from habitat.core.embodied_task import SimulatorTaskAction\n",
    "from habitat.sims.habitat_simulator.actions import (\n",
    "    HabitatSimActions,\n",
    "    HabitatSimV1ActionSpaceConfiguration,\n",
    ")\n",
    "from habitat_sim.agent.controls.controls import ActuationSpec\n",
    "from habitat_sim.physics import MotionType\n",
    "\n",
    "\n",
    "# @markdown For instance, `GrabReleaseActuationSpec` contains the following:\n",
    "# @markdown - `visual_sensor_name` defines which viewport (rgb, depth, etc) to to use to cast the ray.\n",
    "# @markdown - `crosshair_pos` stores the position in the viewport through which the ray passes. Any object which intersects with this ray can be grabbed by the agent.\n",
    "# @markdown - `amount` defines a distance threshold. Objects which are farther than the treshold cannot be picked up by the agent.\n",
    "@attr.s(auto_attribs=True, slots=True)\n",
    "class GrabReleaseActuationSpec(ActuationSpec):\n",
    "    visual_sensor_name: str = \"rgb\"\n",
    "    crosshair_pos: List[int] = [128, 128]\n",
    "    amount: float = 2.0\n",
    "\n",
    "\n",
    "# @markdown Then, we extend the `HabitatSimV1ActionSpaceConfiguration` to add the above action into the agent's action space. `ActionSpaceConfiguration` is a mapping between action name and the corresponding `ActionSpec`\n",
    "@registry.register_action_space_configuration(name=\"RearrangementActions-v0\")\n",
    "class RearrangementSimV0ActionSpaceConfiguration(\n",
    "    HabitatSimV1ActionSpaceConfiguration\n",
    "):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if not HabitatSimActions.has_action(\"GRAB_RELEASE\"):\n",
    "            HabitatSimActions.extend_action_space(\"GRAB_RELEASE\")\n",
    "\n",
    "    def get(self):\n",
    "        config = super().get()\n",
    "        new_config = {\n",
    "            HabitatSimActions.GRAB_RELEASE: habitat_sim.ActionSpec(\n",
    "                \"grab_or_release_object_under_crosshair\",\n",
    "                GrabReleaseActuationSpec(\n",
    "                    visual_sensor_name=self.config.VISUAL_SENSOR,\n",
    "                    crosshair_pos=self.config.CROSSHAIR_POS,\n",
    "                    amount=self.config.GRAB_DISTANCE,\n",
    "                ),\n",
    "            )\n",
    "        }\n",
    "\n",
    "        config.update(new_config)\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "# @markdown Finally, we extend `SimualtorTaskAction` which tells the simulator which action to call when a named action ('GRAB_RELEASE' in this case) is predicte by the agent's policy.\n",
    "@registry.register_task_action\n",
    "class GrabOrReleaseAction(SimulatorTaskAction):\n",
    "    def step(self, *args: Any, **kwargs: Any):\n",
    "        r\"\"\"This method is called from ``Env`` on each ``step``.\"\"\"\n",
    "        return self._sim.step(HabitatSimActions.GRAB_RELEASE)\n",
    "\n",
    "\n",
    "_C.TASK.ACTIONS.GRAB_RELEASE = CN()\n",
    "_C.TASK.ACTIONS.GRAB_RELEASE.TYPE = \"GrabOrReleaseAction\"\n",
    "_C.SIMULATOR.CROSSHAIR_POS = [128, 160]\n",
    "_C.SIMULATOR.GRAB_DISTANCE = 2.0\n",
    "_C.SIMULATOR.VISUAL_SENSOR = \"rgb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Setup Simulator Class for Rearrangement Task\n",
    "\n",
    "![sim](https://drive.google.com/uc?id=1ce6Ti-gpumMEyfomqAKWqOspXm6tN4_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title RearrangementSim Class\n",
    "# @markdown Here we will extend the `HabitatSim` class for the rearrangement task. We will make the following changes:\n",
    "# @markdown - define a new `_initialize_objects` function which will load the object in its initial configuration as defined by the episode.\n",
    "# @markdown - define a `gripped_object_id` property that stores whether the agent is holding any object or not.\n",
    "# @markdown - modify the `step` function of the simulator to use the `grab/release` action we define earlier.\n",
    "\n",
    "# @markdown #### Writing the `step` function:\n",
    "# @markdown Since we added a new action for this task, we have to modify the `step` function to define what happens when `grab/release` action is called. If a simple navigation action (`move_forward`, `turn_left`, `turn_right`) is called, we pass it forward to `act` function of the agent which already defines the behavior of these actions.\n",
    "\n",
    "# @markdown For the `grab/release` action, if the agent is not already holding an object, we first call the `raycast` function using the values from the `ActuationSpec` to see if any object is grippable. If it returns a valid object id, we put the object in a \"invisible\" inventory and remove it from the scene.\n",
    "\n",
    "# @markdown If the agent was already holding an object, `grab/release` action will try release the object at the same relative position as it was grabbed. If the object can be placed without any collision, then the `release` action is successful.\n",
    "\n",
    "from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim\n",
    "from habitat_sim.nav import NavMeshSettings\n",
    "from habitat_sim.utils.common import quat_from_coeffs, quat_to_magnum\n",
    "\n",
    "\n",
    "@registry.register_simulator(name=\"RearrangementSim-v0\")\n",
    "class RearrangementSim(HabitatSim):\n",
    "    r\"\"\"Simulator wrapper over habitat-sim with\n",
    "    object rearrangement functionalities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        self.did_reset = False\n",
    "        super().__init__(config=config)\n",
    "        self.grip_offset = np.eye(4)\n",
    "\n",
    "        agent_id = self.habitat_config.DEFAULT_AGENT_ID\n",
    "        agent_config = self._get_agent_config(agent_id)\n",
    "\n",
    "        self.navmesh_settings = NavMeshSettings()\n",
    "        self.navmesh_settings.set_defaults()\n",
    "        self.navmesh_settings.agent_radius = agent_config.RADIUS\n",
    "        self.navmesh_settings.agent_height = agent_config.HEIGHT\n",
    "\n",
    "    def reconfigure(self, config: Config) -> None:\n",
    "        super().reconfigure(config)\n",
    "        self._initialize_objects()\n",
    "\n",
    "    def reset(self):\n",
    "        sim_obs = super().reset()\n",
    "        if self._update_agents_state():\n",
    "            sim_obs = self.get_sensor_observations()\n",
    "\n",
    "        self._prev_sim_obs = sim_obs\n",
    "        self.did_reset = True\n",
    "        self.grip_offset = np.eye(4)\n",
    "        return self._sensor_suite.get_observations(sim_obs)\n",
    "\n",
    "    def _initialize_objects(self):\n",
    "        objects = self.habitat_config.objects[0]\n",
    "        obj_attr_mgr = self.get_object_template_manager()\n",
    "        obj_attr_mgr.load_configs(\n",
    "            str(os.path.join(data_path, \"test_assets/objects\"))\n",
    "        )\n",
    "        # first remove all existing objects\n",
    "        existing_object_ids = self.get_existing_object_ids()\n",
    "\n",
    "        if len(existing_object_ids) > 0:\n",
    "            for obj_id in existing_object_ids:\n",
    "                self.remove_object(obj_id)\n",
    "\n",
    "        self.sim_object_to_objid_mapping = {}\n",
    "        self.objid_to_sim_object_mapping = {}\n",
    "\n",
    "        if objects is not None:\n",
    "            object_template = objects[\"object_template\"]\n",
    "            object_pos = objects[\"position\"]\n",
    "            object_rot = objects[\"rotation\"]\n",
    "\n",
    "            object_template_id = obj_attr_mgr.load_object_configs(\n",
    "                object_template\n",
    "            )[0]\n",
    "            object_attr = obj_attr_mgr.get_template_by_ID(object_template_id)\n",
    "            obj_attr_mgr.register_template(object_attr)\n",
    "\n",
    "            object_id = self.add_object_by_handle(object_attr.handle)\n",
    "            self.sim_object_to_objid_mapping[object_id] = objects[\"object_id\"]\n",
    "            self.objid_to_sim_object_mapping[objects[\"object_id\"]] = object_id\n",
    "\n",
    "            self.set_translation(object_pos, object_id)\n",
    "            if isinstance(object_rot, list):\n",
    "                object_rot = quat_from_coeffs(object_rot)\n",
    "\n",
    "            object_rot = quat_to_magnum(object_rot)\n",
    "            self.set_rotation(object_rot, object_id)\n",
    "\n",
    "            self.set_object_motion_type(MotionType.STATIC, object_id)\n",
    "\n",
    "        # Recompute the navmesh after placing all the objects.\n",
    "        self.recompute_navmesh(self.pathfinder, self.navmesh_settings, True)\n",
    "\n",
    "    def _sync_gripped_object(self, gripped_object_id):\n",
    "        r\"\"\"\n",
    "        Sync the gripped object with the object associated with the agent.\n",
    "        \"\"\"\n",
    "        if gripped_object_id != -1:\n",
    "            agent_body_transformation = (\n",
    "                self._default_agent.scene_node.transformation\n",
    "            )\n",
    "            self.set_transformation(\n",
    "                agent_body_transformation, gripped_object_id\n",
    "            )\n",
    "            translation = agent_body_transformation.transform_point(\n",
    "                np.array([0, 2.0, 0])\n",
    "            )\n",
    "            self.set_translation(translation, gripped_object_id)\n",
    "\n",
    "    @property\n",
    "    def gripped_object_id(self):\n",
    "        return self._prev_sim_obs.get(\"gripped_object_id\", -1)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        dt = 1 / 60.0\n",
    "        self._num_total_frames += 1\n",
    "        collided = False\n",
    "        gripped_object_id = self.gripped_object_id\n",
    "\n",
    "        agent_config = self._default_agent.agent_config\n",
    "        action_spec = agent_config.action_space[action]\n",
    "\n",
    "        if action_spec.name == \"grab_or_release_object_under_crosshair\":\n",
    "            # If already holding an agent\n",
    "            if gripped_object_id != -1:\n",
    "                agent_body_transformation = (\n",
    "                    self._default_agent.scene_node.transformation\n",
    "                )\n",
    "                T = np.dot(agent_body_transformation, self.grip_offset)\n",
    "\n",
    "                self.set_transformation(T, gripped_object_id)\n",
    "\n",
    "                position = self.get_translation(gripped_object_id)\n",
    "\n",
    "                if self.pathfinder.is_navigable(position):\n",
    "                    self.set_object_motion_type(\n",
    "                        MotionType.STATIC, gripped_object_id\n",
    "                    )\n",
    "                    gripped_object_id = -1\n",
    "                    self.recompute_navmesh(\n",
    "                        self.pathfinder, self.navmesh_settings, True\n",
    "                    )\n",
    "            # if not holding an object, then try to grab\n",
    "            else:\n",
    "                gripped_object_id = raycast(\n",
    "                    self,\n",
    "                    action_spec.actuation.visual_sensor_name,\n",
    "                    crosshair_pos=action_spec.actuation.crosshair_pos,\n",
    "                    max_distance=action_spec.actuation.amount,\n",
    "                )\n",
    "\n",
    "                # found a grabbable object.\n",
    "                if gripped_object_id != -1:\n",
    "                    agent_body_transformation = (\n",
    "                        self._default_agent.scene_node.transformation\n",
    "                    )\n",
    "\n",
    "                    self.grip_offset = np.dot(\n",
    "                        np.array(agent_body_transformation.inverted()),\n",
    "                        np.array(self.get_transformation(gripped_object_id)),\n",
    "                    )\n",
    "                    self.set_object_motion_type(\n",
    "                        MotionType.KINEMATIC, gripped_object_id\n",
    "                    )\n",
    "                    self.recompute_navmesh(\n",
    "                        self.pathfinder, self.navmesh_settings, True\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            collided = self._default_agent.act(action)\n",
    "            self._last_state = self._default_agent.get_state()\n",
    "\n",
    "        # step physics by dt\n",
    "        super().step_world(dt)\n",
    "\n",
    "        # Sync the gripped object after the agent moves.\n",
    "        self._sync_gripped_object(gripped_object_id)\n",
    "\n",
    "        # obtain observations\n",
    "        self._prev_sim_obs = self.get_sensor_observations()\n",
    "        self._prev_sim_obs[\"collided\"] = collided\n",
    "        self._prev_sim_obs[\"gripped_object_id\"] = gripped_object_id\n",
    "\n",
    "        observations = self._sensor_suite.get_observations(self._prev_sim_obs)\n",
    "        return observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Rearrangement Task\n",
    "![task](https://drive.google.com/uc?id=1N75Mmi6aigh33uL765ljsAqLzFmcs7Zn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Implement new sensors and measurements\n",
    "# @markdown After defining the dataset, action space and simulator functions for the rearrangement task, we are one step closer to training agents to solve this task.\n",
    "\n",
    "# @markdown Here we define inputs to the policy and other measurements required to design reward functions.\n",
    "\n",
    "# @markdown **Sensors**: These define various part of the simulator state that's visible to the agent. For simplicity, we'll assume that agent knows the object's current position, object's final goal position relative to the agent's current position.\n",
    "# @markdown - Object's current position will be made given by the `ObjectPosition` sensor\n",
    "# @markdown - Object's goal position will be available through the `ObjectGoal` sensor.\n",
    "# @markdown - Finally, we will also use `GrippedObject` sensor to tell the agent if it's holding any object or not.\n",
    "\n",
    "# @markdown **Measures**: These define various metrics about the task which can be used to measure task progress and define rewards. Note that measurements are *privileged* information not accessible to the agent as part of the observation space. We will need the following measurements:\n",
    "# @markdown - `AgentToObjectDistance` which measure the euclidean distance between the agent and the object.\n",
    "# @markdown - `ObjectToGoalDistance` which measures the euclidean distance between the object and the goal.\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "import habitat_sim\n",
    "from habitat.config.default import CN, Config\n",
    "from habitat.core.dataset import Episode\n",
    "from habitat.core.embodied_task import Measure\n",
    "from habitat.core.simulator import Observations, Sensor, SensorTypes, Simulator\n",
    "from habitat.tasks.nav.nav import PointGoalSensor\n",
    "\n",
    "\n",
    "@registry.register_sensor\n",
    "class GrippedObjectSensor(Sensor):\n",
    "    cls_uuid = \"gripped_object_id\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *args: Any, sim: RearrangementSim, config: Config, **kwargs: Any\n",
    "    ):\n",
    "        self._sim = sim\n",
    "        super().__init__(config=config)\n",
    "\n",
    "    def _get_uuid(self, *args: Any, **kwargs: Any) -> str:\n",
    "        return self.cls_uuid\n",
    "\n",
    "    def _get_observation_space(self, *args: Any, **kwargs: Any):\n",
    "\n",
    "        return spaces.Discrete(len(self._sim.get_existing_object_ids()))\n",
    "\n",
    "    def _get_sensor_type(self, *args: Any, **kwargs: Any):\n",
    "        return SensorTypes.MEASUREMENT\n",
    "\n",
    "    def get_observation(\n",
    "        self,\n",
    "        observations: Dict[str, Observations],\n",
    "        episode: Episode,\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        obj_id = self._sim.sim_object_to_objid_mapping.get(\n",
    "            self._sim.gripped_object_id, -1\n",
    "        )\n",
    "        return obj_id\n",
    "\n",
    "\n",
    "@registry.register_sensor\n",
    "class ObjectPosition(PointGoalSensor):\n",
    "    cls_uuid: str = \"object_position\"\n",
    "\n",
    "    def _get_observation_space(self, *args: Any, **kwargs: Any):\n",
    "        sensor_shape = (self._dimensionality,)\n",
    "\n",
    "        return spaces.Box(\n",
    "            low=np.finfo(np.float32).min,\n",
    "            high=np.finfo(np.float32).max,\n",
    "            shape=sensor_shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def get_observation(\n",
    "        self, *args: Any, observations, episode, **kwargs: Any\n",
    "    ):\n",
    "        agent_state = self._sim.get_agent_state()\n",
    "        agent_position = agent_state.position\n",
    "        rotation_world_agent = agent_state.rotation\n",
    "\n",
    "        object_id = self._sim.get_existing_object_ids()[0]\n",
    "        object_position = self._sim.get_translation(object_id)\n",
    "        pointgoal = self._compute_pointgoal(\n",
    "            agent_position, rotation_world_agent, object_position\n",
    "        )\n",
    "        return pointgoal\n",
    "\n",
    "\n",
    "@registry.register_sensor\n",
    "class ObjectGoal(PointGoalSensor):\n",
    "    cls_uuid: str = \"object_goal\"\n",
    "\n",
    "    def _get_observation_space(self, *args: Any, **kwargs: Any):\n",
    "        sensor_shape = (self._dimensionality,)\n",
    "\n",
    "        return spaces.Box(\n",
    "            low=np.finfo(np.float32).min,\n",
    "            high=np.finfo(np.float32).max,\n",
    "            shape=sensor_shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def get_observation(\n",
    "        self, *args: Any, observations, episode, **kwargs: Any\n",
    "    ):\n",
    "        agent_state = self._sim.get_agent_state()\n",
    "        agent_position = agent_state.position\n",
    "        rotation_world_agent = agent_state.rotation\n",
    "\n",
    "        goal_position = np.array(episode.goals.position, dtype=np.float32)\n",
    "\n",
    "        point_goal = self._compute_pointgoal(\n",
    "            agent_position, rotation_world_agent, goal_position\n",
    "        )\n",
    "        return point_goal\n",
    "\n",
    "\n",
    "@registry.register_measure\n",
    "class ObjectToGoalDistance(Measure):\n",
    "    \"\"\"The measure calculates distance of object towards the goal.\"\"\"\n",
    "\n",
    "    cls_uuid: str = \"object_to_goal_distance\"\n",
    "\n",
    "    def __init__(\n",
    "        self, sim: Simulator, config: Config, *args: Any, **kwargs: Any\n",
    "    ):\n",
    "        self._sim = sim\n",
    "        self._config = config\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_uuid(*args: Any, **kwargs: Any):\n",
    "        return ObjectToGoalDistance.cls_uuid\n",
    "\n",
    "    def reset_metric(self, episode, *args: Any, **kwargs: Any):\n",
    "        self.update_metric(*args, episode=episode, **kwargs)\n",
    "\n",
    "    def _geo_dist(self, src_pos, goal_pos: np.array) -> float:\n",
    "        return self._sim.geodesic_distance(src_pos, [goal_pos])\n",
    "\n",
    "    def _euclidean_distance(self, position_a, position_b):\n",
    "        return np.linalg.norm(\n",
    "            np.array(position_b) - np.array(position_a), ord=2\n",
    "        )\n",
    "\n",
    "    def update_metric(self, episode, *args: Any, **kwargs: Any):\n",
    "        sim_obj_id = self._sim.get_existing_object_ids()[0]\n",
    "\n",
    "        previous_position = np.array(\n",
    "            self._sim.get_translation(sim_obj_id)\n",
    "        ).tolist()\n",
    "        goal_position = episode.goals.position\n",
    "        self._metric = self._euclidean_distance(\n",
    "            previous_position, goal_position\n",
    "        )\n",
    "\n",
    "\n",
    "@registry.register_measure\n",
    "class AgentToObjectDistance(Measure):\n",
    "    \"\"\"The measure calculates the distance of objects from the agent\"\"\"\n",
    "\n",
    "    cls_uuid: str = \"agent_to_object_distance\"\n",
    "\n",
    "    def __init__(\n",
    "        self, sim: Simulator, config: Config, *args: Any, **kwargs: Any\n",
    "    ):\n",
    "        self._sim = sim\n",
    "        self._config = config\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_uuid(*args: Any, **kwargs: Any):\n",
    "        return AgentToObjectDistance.cls_uuid\n",
    "\n",
    "    def reset_metric(self, episode, *args: Any, **kwargs: Any):\n",
    "        self.update_metric(*args, episode=episode, **kwargs)\n",
    "\n",
    "    def _euclidean_distance(self, position_a, position_b):\n",
    "        return np.linalg.norm(\n",
    "            np.array(position_b) - np.array(position_a), ord=2\n",
    "        )\n",
    "\n",
    "    def update_metric(self, episode, *args: Any, **kwargs: Any):\n",
    "        sim_obj_id = self._sim.get_existing_object_ids()[0]\n",
    "        previous_position = np.array(\n",
    "            self._sim.get_translation(sim_obj_id)\n",
    "        ).tolist()\n",
    "\n",
    "        agent_state = self._sim.get_agent_state()\n",
    "        agent_position = agent_state.position\n",
    "\n",
    "        self._metric = self._euclidean_distance(\n",
    "            previous_position, agent_position\n",
    "        )\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# # REARRANGEMENT TASK GRIPPED OBJECT SENSOR\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.TASK.GRIPPED_OBJECT_SENSOR = CN()\n",
    "_C.TASK.GRIPPED_OBJECT_SENSOR.TYPE = \"GrippedObjectSensor\"\n",
    "# -----------------------------------------------------------------------------\n",
    "# # REARRANGEMENT TASK ALL OBJECT POSITIONS SENSOR\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.TASK.OBJECT_POSITION = CN()\n",
    "_C.TASK.OBJECT_POSITION.TYPE = \"ObjectPosition\"\n",
    "_C.TASK.OBJECT_POSITION.GOAL_FORMAT = \"POLAR\"\n",
    "_C.TASK.OBJECT_POSITION.DIMENSIONALITY = 2\n",
    "# -----------------------------------------------------------------------------\n",
    "# # REARRANGEMENT TASK ALL OBJECT GOALS SENSOR\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.TASK.OBJECT_GOAL = CN()\n",
    "_C.TASK.OBJECT_GOAL.TYPE = \"ObjectGoal\"\n",
    "_C.TASK.OBJECT_GOAL.GOAL_FORMAT = \"POLAR\"\n",
    "_C.TASK.OBJECT_GOAL.DIMENSIONALITY = 2\n",
    "# -----------------------------------------------------------------------------\n",
    "# # OBJECT_DISTANCE_TO_GOAL MEASUREMENT\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.TASK.OBJECT_TO_GOAL_DISTANCE = CN()\n",
    "_C.TASK.OBJECT_TO_GOAL_DISTANCE.TYPE = \"ObjectToGoalDistance\"\n",
    "# -----------------------------------------------------------------------------\n",
    "# # OBJECT_DISTANCE_FROM_AGENT MEASUREMENT\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.TASK.AGENT_TO_OBJECT_DISTANCE = CN()\n",
    "_C.TASK.AGENT_TO_OBJECT_DISTANCE.TYPE = \"AgentToObjectDistance\"\n",
    "\n",
    "from habitat.config.default import CN, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define `RearrangementTask` by extending `NavigationTask`\n",
    "from habitat.tasks.nav.nav import NavigationTask, merge_sim_episode_config\n",
    "\n",
    "\n",
    "def merge_sim_episode_with_object_config(\n",
    "    sim_config: Config, episode: Type[Episode]\n",
    ") -> Any:\n",
    "    sim_config = merge_sim_episode_config(sim_config, episode)\n",
    "    sim_config.defrost()\n",
    "    sim_config.objects = [episode.objects.__dict__]\n",
    "    sim_config.freeze()\n",
    "\n",
    "    return sim_config\n",
    "\n",
    "\n",
    "@registry.register_task(name=\"RearrangementTask-v0\")\n",
    "class RearrangementTask(NavigationTask):\n",
    "    r\"\"\"Embodied Rearrangement Task\n",
    "    Goal: An agent must place objects at their corresponding goal position.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def overwrite_sim_config(self, sim_config, episode):\n",
    "        return merge_sim_episode_with_object_config(sim_config, episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a hard-coded and an RL agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load the `RearrangementTask` in Habitat-Lab and run a hard-coded agent\n",
    "import habitat\n",
    "\n",
    "config = habitat.get_config(\"configs/tasks/pointnav.yaml\")\n",
    "config.defrost()\n",
    "config.ENVIRONMENT.MAX_EPISODE_STEPS = 50\n",
    "config.SIMULATOR.TYPE = \"RearrangementSim-v0\"\n",
    "config.SIMULATOR.ACTION_SPACE_CONFIG = \"RearrangementActions-v0\"\n",
    "config.SIMULATOR.GRAB_DISTANCE = 2.0\n",
    "config.SIMULATOR.HABITAT_SIM_V0.ENABLE_PHYSICS = True\n",
    "config.TASK.TYPE = \"RearrangementTask-v0\"\n",
    "config.TASK.SUCCESS_DISTANCE = 1.0\n",
    "config.TASK.SENSORS = [\n",
    "    \"GRIPPED_OBJECT_SENSOR\",\n",
    "    \"OBJECT_POSITION\",\n",
    "    \"OBJECT_GOAL\",\n",
    "]\n",
    "config.TASK.GOAL_SENSOR_UUID = \"object_goal\"\n",
    "config.TASK.MEASUREMENTS = [\n",
    "    \"OBJECT_TO_GOAL_DISTANCE\",\n",
    "    \"AGENT_TO_OBJECT_DISTANCE\",\n",
    "]\n",
    "config.TASK.POSSIBLE_ACTIONS = [\"STOP\", \"MOVE_FORWARD\", \"GRAB_RELEASE\"]\n",
    "config.DATASET.TYPE = \"RearrangementDataset-v0\"\n",
    "config.DATASET.SPLIT = \"train\"\n",
    "config.DATASET.DATA_PATH = (\n",
    "    \"data/datasets/rearrangement/coda/v1/{split}/{split}.json.gz\"\n",
    ")\n",
    "config.freeze()\n",
    "\n",
    "\n",
    "def print_info(obs, metrics):\n",
    "    print(\n",
    "        \"Gripped Object: {}, Distance To Object: {}, Distance To Goal: {}\".format(\n",
    "            obs[\"gripped_object_id\"],\n",
    "            metrics[\"agent_to_object_distance\"],\n",
    "            metrics[\"object_to_goal_distance\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "try:  # Got to make initialization idiot proof\n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "with habitat.Env(config) as env:\n",
    "    obs = env.reset()\n",
    "    obs_list = []\n",
    "    # Get closer to the object\n",
    "    while True:\n",
    "        obs = env.step(1)\n",
    "        obs_list.append(obs)\n",
    "        metrics = env.get_metrics()\n",
    "        print_info(obs, metrics)\n",
    "        if metrics[\"agent_to_object_distance\"] < 2.0:\n",
    "            break\n",
    "\n",
    "    # Grab the object\n",
    "    obs = env.step(2)\n",
    "    obs_list.append(obs)\n",
    "    metrics = env.get_metrics()\n",
    "    print_info(obs, metrics)\n",
    "    assert obs[\"gripped_object_id\"] != -1\n",
    "\n",
    "    # Get closer to the goal\n",
    "    while True:\n",
    "        obs = env.step(1)\n",
    "        obs_list.append(obs)\n",
    "        metrics = env.get_metrics()\n",
    "        print_info(obs, metrics)\n",
    "        if metrics[\"object_to_goal_distance\"] < 2.0:\n",
    "            break\n",
    "\n",
    "    # Release the object\n",
    "    obs = env.step(2)\n",
    "    obs_list.append(obs)\n",
    "    metrics = env.get_metrics()\n",
    "    print_info(obs, metrics)\n",
    "    assert obs[\"gripped_object_id\"] == -1\n",
    "\n",
    "    if make_video:\n",
    "        make_video_cv2(\n",
    "            obs_list,\n",
    "            [190, 128],\n",
    "            \"hard-coded-agent\",\n",
    "            fps=5.0,\n",
    "            open_vid=show_video,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create a task specific RL Environment with a new reward definition.\n",
    "# @markdown We create a `RearragenmentRLEnv` class and modify the `get_reward()` function.\n",
    "# @markdown The reward sturcture is as follows:\n",
    "# @markdown - The agent gets a positive reward if the agent gets closer to the object otherwise a negative reward.\n",
    "# @markdown - The agent gets a positive reward if it moves the object closer to goal otherwise a negative reward.\n",
    "# @markdown - The agent gets a positive reward when the agent \"picks\" up an object for the first time. For all other \"grab/release\" action, it gets a negative reward.\n",
    "# @markdown - The agent gets a slack penalty of -0.01 for every action it takes in the environment.\n",
    "# @markdown - Finally the agent gets a large success reward when the episode is completed successfully.\n",
    "\n",
    "from typing import Optional, Type\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import habitat\n",
    "from habitat import Config, Dataset\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "from habitat_baselines.common.environments import NavRLEnv\n",
    "\n",
    "\n",
    "@baseline_registry.register_env(name=\"RearrangementRLEnv\")\n",
    "class RearrangementRLEnv(NavRLEnv):\n",
    "    def __init__(self, config: Config, dataset: Optional[Dataset] = None):\n",
    "        self._prev_measure = {\n",
    "            \"agent_to_object_distance\": 0.0,\n",
    "            \"object_to_goal_distance\": 0.0,\n",
    "            \"gripped_object_id\": -1,\n",
    "            \"gripped_object_count\": 0,\n",
    "        }\n",
    "\n",
    "        super().__init__(config, dataset)\n",
    "\n",
    "        self._success_distance = self._core_env_config.TASK.SUCCESS_DISTANCE\n",
    "\n",
    "    def reset(self):\n",
    "        self._previous_action = None\n",
    "        observations = super().reset()\n",
    "\n",
    "        self._prev_measure.update(self.habitat_env.get_metrics())\n",
    "        self._prev_measure[\"gripped_object_id\"] = -1\n",
    "        self._prev_measure[\"gripped_object_count\"] = 0\n",
    "\n",
    "        return observations\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        self._previous_action = kwargs[\"action\"]\n",
    "        return super().step(*args, **kwargs)\n",
    "\n",
    "    def get_reward_range(self):\n",
    "        return (\n",
    "            self._rl_config.SLACK_REWARD - 1.0,\n",
    "            self._rl_config.SUCCESS_REWARD + 1.0,\n",
    "        )\n",
    "\n",
    "    def get_reward(self, observations):\n",
    "        reward = self._rl_config.SLACK_REWARD\n",
    "        gripped_success_reward = 0.0\n",
    "        episode_success_reward = 0.0\n",
    "        agent_to_object_dist_reward = 0.0\n",
    "        object_to_goal_dist_reward = 0.0\n",
    "\n",
    "        action_name = self._env.task.get_action_name(\n",
    "            self._previous_action[\"action\"]\n",
    "        )\n",
    "\n",
    "        # If object grabbed, add a success reward\n",
    "        # The reward gets awarded only once for an object.\n",
    "        if (\n",
    "            action_name == \"GRAB_RELEASE\"\n",
    "            and observations[\"gripped_object_id\"] >= 0\n",
    "        ):\n",
    "            obj_id = observations[\"gripped_object_id\"]\n",
    "            self._prev_measure[\"gripped_object_count\"] += 1\n",
    "\n",
    "            gripped_success_reward = (\n",
    "                self._rl_config.GRIPPED_SUCCESS_REWARD\n",
    "                if self._prev_measure[\"gripped_object_count\"] == 1\n",
    "                else 0.0\n",
    "            )\n",
    "        # add a penalty everytime grab/action is called and doesn't do anything\n",
    "        elif action_name == \"GRAB_RELEASE\":\n",
    "            gripped_success_reward += -0.1\n",
    "\n",
    "        self._prev_measure[\"gripped_object_id\"] = observations[\n",
    "            \"gripped_object_id\"\n",
    "        ]\n",
    "\n",
    "        # If the action is not a grab/release action, and the agent\n",
    "        # has not picked up an object, then give reward based on agent to\n",
    "        # object distance.\n",
    "        if (\n",
    "            action_name != \"GRAB_RELEASE\"\n",
    "            and self._prev_measure[\"gripped_object_id\"] == -1\n",
    "        ):\n",
    "            agent_to_object_dist_reward = self.get_agent_to_object_dist_reward(\n",
    "                observations\n",
    "            )\n",
    "\n",
    "        # If the action is not a grab/release action, and the agent\n",
    "        # has picked up an object, then give reward based on object to\n",
    "        # to goal distance.\n",
    "        if (\n",
    "            action_name != \"GRAB_RELEASE\"\n",
    "            and self._prev_measure[\"gripped_object_id\"] != -1\n",
    "        ):\n",
    "            object_to_goal_dist_reward = self.get_object_to_goal_dist_reward()\n",
    "\n",
    "        if (\n",
    "            self._episode_success(observations)\n",
    "            and self._prev_measure[\"gripped_object_id\"] == -1\n",
    "            and action_name == \"STOP\"\n",
    "        ):\n",
    "            episode_success_reward = self._rl_config.SUCCESS_REWARD\n",
    "\n",
    "        reward += (\n",
    "            agent_to_object_dist_reward\n",
    "            + object_to_goal_dist_reward\n",
    "            + gripped_success_reward\n",
    "            + episode_success_reward\n",
    "        )\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_agent_to_object_dist_reward(self, observations):\n",
    "        \"\"\"\n",
    "        Encourage the agent to move towards the closest object which is not already in place.\n",
    "        \"\"\"\n",
    "        curr_metric = self._env.get_metrics()[\"agent_to_object_distance\"]\n",
    "        prev_metric = self._prev_measure[\"agent_to_object_distance\"]\n",
    "        dist_reward = prev_metric - curr_metric\n",
    "\n",
    "        self._prev_measure[\"agent_to_object_distance\"] = curr_metric\n",
    "\n",
    "        return dist_reward\n",
    "\n",
    "    def get_object_to_goal_dist_reward(self):\n",
    "        curr_metric = self._env.get_metrics()[\"object_to_goal_distance\"]\n",
    "        prev_metric = self._prev_measure[\"object_to_goal_distance\"]\n",
    "        dist_reward = prev_metric - curr_metric\n",
    "\n",
    "        self._prev_measure[\"object_to_goal_distance\"] = curr_metric\n",
    "\n",
    "        return dist_reward\n",
    "\n",
    "    def _episode_success(self, observations):\n",
    "        r\"\"\"Returns True if object is within distance threshold of the goal.\"\"\"\n",
    "        dist = self._env.get_metrics()[\"object_to_goal_distance\"]\n",
    "        if (\n",
    "            abs(dist) > self._success_distance\n",
    "            or observations[\"gripped_object_id\"] != -1\n",
    "        ):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _gripped_success(self, observations):\n",
    "        if (\n",
    "            observations[\"gripped_object_id\"] >= 0\n",
    "            and observations[\"gripped_object_id\"]\n",
    "            != self._prev_measure[\"gripped_object_id\"]\n",
    "        ):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_done(self, observations):\n",
    "        done = False\n",
    "        action_name = self._env.task.get_action_name(\n",
    "            self._previous_action[\"action\"]\n",
    "        )\n",
    "        if self._env.episode_over or (\n",
    "            self._episode_success(observations)\n",
    "            and self._prev_measure[\"gripped_object_id\"] == -1\n",
    "            and action_name == \"STOP\"\n",
    "        ):\n",
    "            done = True\n",
    "        return done\n",
    "\n",
    "    def get_info(self, observations):\n",
    "        info = self.habitat_env.get_metrics()\n",
    "        info[\"episode_success\"] = self._episode_success(observations)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from habitat import Config, logger\n",
    "from habitat.utils.visualizations.utils import observations_to_image\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "from habitat_baselines.common.environments import get_env_class\n",
    "from habitat_baselines.common.tensorboard_utils import TensorboardWriter\n",
    "from habitat_baselines.rl.models.rnn_state_encoder import (\n",
    "    build_rnn_state_encoder,\n",
    ")\n",
    "from habitat_baselines.rl.ppo import PPO\n",
    "from habitat_baselines.rl.ppo.policy import Net, Policy\n",
    "from habitat_baselines.rl.ppo.ppo_trainer import PPOTrainer\n",
    "from habitat_baselines.utils.common import batch_obs, generate_video\n",
    "from habitat_baselines.utils.env_utils import make_env_fn\n",
    "\n",
    "\n",
    "def construct_envs(\n",
    "    config,\n",
    "    env_class,\n",
    "    workers_ignore_signals=False,\n",
    "):\n",
    "    r\"\"\"Create VectorEnv object with specified config and env class type.\n",
    "    To allow better performance, dataset are split into small ones for\n",
    "    each individual env, grouped by scenes.\n",
    "\n",
    "    :param config: configs that contain num_processes as well as information\n",
    "    :param necessary to create individual environments.\n",
    "    :param env_class: class type of the envs to be created.\n",
    "    :param workers_ignore_signals: Passed to :ref:`habitat.VectorEnv`'s constructor\n",
    "\n",
    "    :return: VectorEnv object created according to specification.\n",
    "    \"\"\"\n",
    "\n",
    "    num_processes = config.NUM_ENVIRONMENTS\n",
    "    configs = []\n",
    "    env_classes = [env_class for _ in range(num_processes)]\n",
    "    dataset = habitat.datasets.make_dataset(config.TASK_CONFIG.DATASET.TYPE)\n",
    "    scenes = config.TASK_CONFIG.DATASET.CONTENT_SCENES\n",
    "    if \"*\" in config.TASK_CONFIG.DATASET.CONTENT_SCENES:\n",
    "        scenes = dataset.get_scenes_to_load(config.TASK_CONFIG.DATASET)\n",
    "\n",
    "    if num_processes > 1:\n",
    "        if len(scenes) == 0:\n",
    "            raise RuntimeError(\n",
    "                \"No scenes to load, multiple process logic relies on being able to split scenes uniquely between processes\"\n",
    "            )\n",
    "\n",
    "        if len(scenes) < num_processes:\n",
    "            scenes = scenes * num_processes\n",
    "\n",
    "        random.shuffle(scenes)\n",
    "\n",
    "    scene_splits = [[] for _ in range(num_processes)]\n",
    "    for idx, scene in enumerate(scenes):\n",
    "        scene_splits[idx % len(scene_splits)].append(scene)\n",
    "\n",
    "    assert sum(map(len, scene_splits)) == len(scenes)\n",
    "\n",
    "    for i in range(num_processes):\n",
    "        proc_config = config.clone()\n",
    "        proc_config.defrost()\n",
    "\n",
    "        task_config = proc_config.TASK_CONFIG\n",
    "        task_config.SEED = task_config.SEED + i\n",
    "        if len(scenes) > 0:\n",
    "            task_config.DATASET.CONTENT_SCENES = scene_splits[i]\n",
    "\n",
    "        task_config.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID = (\n",
    "            config.SIMULATOR_GPU_ID\n",
    "        )\n",
    "\n",
    "        task_config.SIMULATOR.AGENT_0.SENSORS = config.SENSORS\n",
    "\n",
    "        proc_config.freeze()\n",
    "        configs.append(proc_config)\n",
    "\n",
    "    envs = habitat.ThreadedVectorEnv(\n",
    "        make_env_fn=make_env_fn,\n",
    "        env_fn_args=tuple(zip(configs, env_classes)),\n",
    "        workers_ignore_signals=workers_ignore_signals,\n",
    "    )\n",
    "    return envs\n",
    "\n",
    "\n",
    "class RearrangementBaselinePolicy(Policy):\n",
    "    def __init__(self, observation_space, action_space, hidden_size=512):\n",
    "        super().__init__(\n",
    "            RearrangementBaselineNet(\n",
    "                observation_space=observation_space, hidden_size=hidden_size\n",
    "            ),\n",
    "            action_space.n,\n",
    "        )\n",
    "\n",
    "    def from_config(cls, config, envs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RearrangementBaselineNet(Net):\n",
    "    r\"\"\"Network which passes the input image through CNN and concatenates\n",
    "    goal vector with CNN's output and passes that through RNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self._n_input_goal = observation_space.spaces[\n",
    "            ObjectGoal.cls_uuid\n",
    "        ].shape[0]\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        self.state_encoder = build_rnn_state_encoder(\n",
    "            2 * self._n_input_goal, self._hidden_size\n",
    "        )\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def is_blind(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def num_recurrent_layers(self):\n",
    "        return self.state_encoder.num_recurrent_layers\n",
    "\n",
    "    def forward(self, observations, rnn_hidden_states, prev_actions, masks):\n",
    "        object_goal_encoding = observations[ObjectGoal.cls_uuid]\n",
    "        object_pos_encoding = observations[ObjectPosition.cls_uuid]\n",
    "\n",
    "        x = [object_goal_encoding, object_pos_encoding]\n",
    "\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x, rnn_hidden_states = self.state_encoder(x, rnn_hidden_states, masks)\n",
    "\n",
    "        return x, rnn_hidden_states\n",
    "\n",
    "\n",
    "@baseline_registry.register_trainer(name=\"ppo-rearrangement\")\n",
    "class RearrangementTrainer(PPOTrainer):\n",
    "    supported_tasks = [\"RearrangementTask-v0\"]\n",
    "\n",
    "    def _setup_actor_critic_agent(self, ppo_cfg: Config) -> None:\n",
    "        r\"\"\"Sets up actor critic and agent for PPO.\n",
    "\n",
    "        Args:\n",
    "            ppo_cfg: config node with relevant params\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        logger.add_filehandler(self.config.LOG_FILE)\n",
    "\n",
    "        self.actor_critic = RearrangementBaselinePolicy(\n",
    "            observation_space=self.envs.observation_spaces[0],\n",
    "            action_space=self.envs.action_spaces[0],\n",
    "            hidden_size=ppo_cfg.hidden_size,\n",
    "        )\n",
    "        self.actor_critic.to(self.device)\n",
    "\n",
    "        self.agent = PPO(\n",
    "            actor_critic=self.actor_critic,\n",
    "            clip_param=ppo_cfg.clip_param,\n",
    "            ppo_epoch=ppo_cfg.ppo_epoch,\n",
    "            num_mini_batch=ppo_cfg.num_mini_batch,\n",
    "            value_loss_coef=ppo_cfg.value_loss_coef,\n",
    "            entropy_coef=ppo_cfg.entropy_coef,\n",
    "            lr=ppo_cfg.lr,\n",
    "            eps=ppo_cfg.eps,\n",
    "            max_grad_norm=ppo_cfg.max_grad_norm,\n",
    "            use_normalized_advantage=ppo_cfg.use_normalized_advantage,\n",
    "        )\n",
    "\n",
    "    def _init_envs(self, config=None):\n",
    "        if config is None:\n",
    "            config = self.config\n",
    "\n",
    "        self.envs = construct_envs(config, get_env_class(config.ENV_NAME))\n",
    "\n",
    "    def train(self) -> None:\n",
    "        r\"\"\"Main method for training PPO.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self._is_distributed:\n",
    "            raise RuntimeError(\"This trainer does not support distributed\")\n",
    "        self._init_train()\n",
    "\n",
    "        count_checkpoints = 0\n",
    "\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=self.agent.optimizer,\n",
    "            lr_lambda=lambda _: 1 - self.percent_done(),\n",
    "        )\n",
    "        ppo_cfg = self.config.RL.PPO\n",
    "\n",
    "        with TensorboardWriter(\n",
    "            self.config.TENSORBOARD_DIR, flush_secs=self.flush_secs\n",
    "        ) as writer:\n",
    "            while not self.is_done():\n",
    "\n",
    "                if ppo_cfg.use_linear_clip_decay:\n",
    "                    self.agent.clip_param = ppo_cfg.clip_param * (\n",
    "                        1 - self.percent_done()\n",
    "                    )\n",
    "\n",
    "                count_steps_delta = 0\n",
    "                for _step in range(ppo_cfg.num_steps):\n",
    "                    count_steps_delta += self._collect_rollout_step()\n",
    "\n",
    "                (\n",
    "                    value_loss,\n",
    "                    action_loss,\n",
    "                    dist_entropy,\n",
    "                ) = self._update_agent()\n",
    "\n",
    "                if ppo_cfg.use_linear_lr_decay:\n",
    "                    lr_scheduler.step()  # type: ignore\n",
    "\n",
    "                losses = self._coalesce_post_step(\n",
    "                    dict(value_loss=value_loss, action_loss=action_loss),\n",
    "                    count_steps_delta,\n",
    "                )\n",
    "                self.num_updates_done += 1\n",
    "\n",
    "                deltas = {\n",
    "                    k: (\n",
    "                        (v[-1] - v[0]).sum().item()\n",
    "                        if len(v) > 1\n",
    "                        else v[0].sum().item()\n",
    "                    )\n",
    "                    for k, v in self.window_episode_stats.items()\n",
    "                }\n",
    "                deltas[\"count\"] = max(deltas[\"count\"], 1.0)\n",
    "\n",
    "                writer.add_scalar(\n",
    "                    \"reward\",\n",
    "                    deltas[\"reward\"] / deltas[\"count\"],\n",
    "                    self.num_steps_done,\n",
    "                )\n",
    "\n",
    "                # Check to see if there are any metrics\n",
    "                # that haven't been logged yet\n",
    "\n",
    "                for k, v in deltas.items():\n",
    "                    if k not in {\"reward\", \"count\"}:\n",
    "                        writer.add_scalar(\n",
    "                            \"metric/\" + k,\n",
    "                            v / deltas[\"count\"],\n",
    "                            self.num_steps_done,\n",
    "                        )\n",
    "\n",
    "                losses = [value_loss, action_loss]\n",
    "                for l, k in zip(losses, [\"value, policy\"]):\n",
    "                    writer.add_scalar(\"losses/\" + k, l, self.num_steps_done)\n",
    "\n",
    "                # log stats\n",
    "                if self.num_updates_done % self.config.LOG_INTERVAL == 0:\n",
    "                    logger.info(\n",
    "                        \"update: {}\\tfps: {:.3f}\\t\".format(\n",
    "                            self.num_updates_done,\n",
    "                            self.num_steps_done / (time.time() - self.t_start),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    logger.info(\n",
    "                        \"update: {}\\tenv-time: {:.3f}s\\tpth-time: {:.3f}s\\t\"\n",
    "                        \"frames: {}\".format(\n",
    "                            self.num_updates_done,\n",
    "                            self.env_time,\n",
    "                            self.pth_time,\n",
    "                            self.num_steps_done,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    logger.info(\n",
    "                        \"Average window size: {}  {}\".format(\n",
    "                            len(self.window_episode_stats[\"count\"]),\n",
    "                            \"  \".join(\n",
    "                                \"{}: {:.3f}\".format(k, v / deltas[\"count\"])\n",
    "                                for k, v in deltas.items()\n",
    "                                if k != \"count\"\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # checkpoint model\n",
    "                if self.should_checkpoint():\n",
    "                    self.save_checkpoint(\n",
    "                        f\"ckpt.{count_checkpoints}.pth\",\n",
    "                        dict(step=self.num_steps_done),\n",
    "                    )\n",
    "                    count_checkpoints += 1\n",
    "\n",
    "            self.envs.close()\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        r\"\"\"Evaluates the current model\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        config = self.config.clone()\n",
    "\n",
    "        if len(self.config.VIDEO_OPTION) > 0:\n",
    "            config.defrost()\n",
    "            config.NUM_ENVIRONMENTS = 1\n",
    "            config.freeze()\n",
    "\n",
    "        logger.info(f\"env config: {config}\")\n",
    "        with construct_envs(config, get_env_class(config.ENV_NAME)) as envs:\n",
    "            observations = envs.reset()\n",
    "            batch = batch_obs(observations, device=self.device)\n",
    "\n",
    "            current_episode_reward = torch.zeros(\n",
    "                envs.num_envs, 1, device=self.device\n",
    "            )\n",
    "            ppo_cfg = self.config.RL.PPO\n",
    "            test_recurrent_hidden_states = torch.zeros(\n",
    "                config.NUM_ENVIRONMENTS,\n",
    "                self.actor_critic.net.num_recurrent_layers,\n",
    "                ppo_cfg.hidden_size,\n",
    "                device=self.device,\n",
    "            )\n",
    "            prev_actions = torch.zeros(\n",
    "                config.NUM_ENVIRONMENTS,\n",
    "                1,\n",
    "                device=self.device,\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "            not_done_masks = torch.zeros(\n",
    "                config.NUM_ENVIRONMENTS,\n",
    "                1,\n",
    "                device=self.device,\n",
    "                dtype=torch.bool,\n",
    "            )\n",
    "\n",
    "            rgb_frames = [\n",
    "                [] for _ in range(self.config.NUM_ENVIRONMENTS)\n",
    "            ]  # type: List[List[np.ndarray]]\n",
    "\n",
    "            if len(config.VIDEO_OPTION) > 0:\n",
    "                os.makedirs(config.VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "            self.actor_critic.eval()\n",
    "\n",
    "            for _i in range(config.TASK_CONFIG.ENVIRONMENT.MAX_EPISODE_STEPS):\n",
    "                current_episodes = envs.current_episodes()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    (\n",
    "                        _,\n",
    "                        actions,\n",
    "                        _,\n",
    "                        test_recurrent_hidden_states,\n",
    "                    ) = self.actor_critic.act(\n",
    "                        batch,\n",
    "                        test_recurrent_hidden_states,\n",
    "                        prev_actions,\n",
    "                        not_done_masks,\n",
    "                        deterministic=False,\n",
    "                    )\n",
    "\n",
    "                    prev_actions.copy_(actions)\n",
    "\n",
    "                outputs = envs.step([a[0].item() for a in actions])\n",
    "\n",
    "                observations, rewards, dones, infos = [\n",
    "                    list(x) for x in zip(*outputs)\n",
    "                ]\n",
    "                batch = batch_obs(observations, device=self.device)\n",
    "\n",
    "                not_done_masks = torch.tensor(\n",
    "                    [[not done] for done in dones],\n",
    "                    dtype=torch.bool,\n",
    "                    device=\"cpu\",\n",
    "                )\n",
    "\n",
    "                rewards = torch.tensor(\n",
    "                    rewards, dtype=torch.float, device=self.device\n",
    "                ).unsqueeze(1)\n",
    "\n",
    "                current_episode_reward += rewards\n",
    "\n",
    "                # episode ended\n",
    "                if not not_done_masks[0].item():\n",
    "                    generate_video(\n",
    "                        video_option=self.config.VIDEO_OPTION,\n",
    "                        video_dir=self.config.VIDEO_DIR,\n",
    "                        images=rgb_frames[0],\n",
    "                        episode_id=current_episodes[0].episode_id,\n",
    "                        checkpoint_idx=0,\n",
    "                        metrics=self._extract_scalars_from_info(infos[0]),\n",
    "                        tb_writer=None,\n",
    "                    )\n",
    "\n",
    "                    print(\"Evaluation Finished.\")\n",
    "                    print(\"Success: {}\".format(infos[0][\"episode_success\"]))\n",
    "                    print(\n",
    "                        \"Reward: {}\".format(current_episode_reward[0].item())\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Distance To Goal: {}\".format(\n",
    "                            infos[0][\"object_to_goal_distance\"]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    return\n",
    "\n",
    "                # episode continues\n",
    "                elif len(self.config.VIDEO_OPTION) > 0:\n",
    "                    frame = observations_to_image(observations[0], infos[0])\n",
    "                    rgb_frames[0].append(frame)\n",
    "\n",
    "                not_done_masks = not_done_masks.to(device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir data/tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train an RL agent on a single episode\n",
    "!if [ -d \"data/tb\" ]; then rm -r data/tb; fi\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import habitat\n",
    "from habitat import Config\n",
    "from habitat_baselines.config.default import get_config as get_baseline_config\n",
    "\n",
    "baseline_config = get_baseline_config(\n",
    "    \"habitat_baselines/config/pointnav/ppo_pointnav.yaml\"\n",
    ")\n",
    "baseline_config.defrost()\n",
    "\n",
    "baseline_config.TASK_CONFIG = config\n",
    "baseline_config.TRAINER_NAME = \"ddppo\"\n",
    "baseline_config.ENV_NAME = \"RearrangementRLEnv\"\n",
    "baseline_config.SIMULATOR_GPU_ID = 0\n",
    "baseline_config.TORCH_GPU_ID = 0\n",
    "baseline_config.VIDEO_OPTION = [\"disk\"]\n",
    "baseline_config.TENSORBOARD_DIR = \"data/tb\"\n",
    "baseline_config.VIDEO_DIR = \"data/videos\"\n",
    "baseline_config.NUM_ENVIRONMENTS = 2\n",
    "baseline_config.SENSORS = [\"RGB_SENSOR\", \"DEPTH_SENSOR\"]\n",
    "baseline_config.CHECKPOINT_FOLDER = \"data/checkpoints\"\n",
    "baseline_config.TOTAL_NUM_STEPS = -1.0\n",
    "\n",
    "if vut.is_notebook():\n",
    "    baseline_config.NUM_UPDATES = 400  # @param {type:\"number\"}\n",
    "else:\n",
    "    baseline_config.NUM_UPDATES = 1\n",
    "\n",
    "baseline_config.LOG_INTERVAL = 10\n",
    "baseline_config.NUM_CHECKPOINTS = 5\n",
    "baseline_config.LOG_FILE = \"data/checkpoints/train.log\"\n",
    "baseline_config.EVAL.SPLIT = \"train\"\n",
    "baseline_config.RL.SUCCESS_REWARD = 2.5  # @param {type:\"number\"}\n",
    "baseline_config.RL.SUCCESS_MEASURE = \"object_to_goal_distance\"\n",
    "baseline_config.RL.REWARD_MEASURE = \"object_to_goal_distance\"\n",
    "baseline_config.RL.GRIPPED_SUCCESS_REWARD = 2.5  # @param {type:\"number\"}\n",
    "\n",
    "baseline_config.freeze()\n",
    "random.seed(baseline_config.TASK_CONFIG.SEED)\n",
    "np.random.seed(baseline_config.TASK_CONFIG.SEED)\n",
    "torch.manual_seed(baseline_config.TASK_CONFIG.SEED)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = RearrangementTrainer(baseline_config)\n",
    "    trainer.train()\n",
    "    trainer.eval()\n",
    "\n",
    "    if make_video:\n",
    "        video_file = os.listdir(\"data/videos\")[0]\n",
    "        vut.display_video(os.path.join(\"data/videos\", video_file))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Habitat Interactive Tasks",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "nb_python//py:percent,colabs//ipynb",
   "notebook_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
