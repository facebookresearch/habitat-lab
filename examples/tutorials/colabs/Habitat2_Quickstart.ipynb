{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This tutorial covers the basics of using Habitat 2.0 including: setting up\n",
    "the environment, creating custom environments, and creating new episode\n",
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a teaser video\n",
    "try:\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    HTML(\n",
    "        '<iframe src=\"https://drive.google.com/file/d/1ltrse38i8pnJPGAXlThylcdy8PMjUMKh/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>'\n",
    "    )\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title Install Dependencies (if on Colab) { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "import os\n",
    "\n",
    "# Colab installation\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    print(\"Setting up Habitat\")\n",
    "    !curl -L https://raw.githubusercontent.com/facebookresearch/habitat-sim/main/examples/colab_utils/colab_install.sh | NIGHTLY=true bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    print(\"Setting Habitat base path\")\n",
    "    %env HABLAB_BASE_CFG_PATH=/content/habitat-lab\n",
    "    import importlib\n",
    "\n",
    "    import PIL\n",
    "\n",
    "    importlib.reload(PIL.TiffTags)\n",
    "\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import numpy as np\n",
    "\n",
    "import habitat\n",
    "import habitat.utils.gym_definitions as habitat_gym\n",
    "from habitat.core.embodied_task import Measure\n",
    "from habitat.core.registry import registry\n",
    "from habitat.core.simulator import Sensor, SensorTypes\n",
    "from habitat.tasks.rearrange.rearrange_sensors import RearrangeReward\n",
    "from habitat.tasks.rearrange.rearrange_task import RearrangeTask\n",
    "from habitat.utils.render_wrapper import overlay_frame\n",
    "from habitat.utils.visualizations.utils import observations_to_image\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "# Quiet the Habitat simulator logging\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n",
    "\n",
    "\n",
    "def insert_render_options(config):\n",
    "    # Added settings to make rendering higher resolution for better visualization\n",
    "    config.defrost()\n",
    "    config.SIMULATOR.THIRD_RGB_SENSOR.WIDTH = 512\n",
    "    config.SIMULATOR.THIRD_RGB_SENSOR.HEIGHT = 512\n",
    "    config.SIMULATOR.CONCUR_RENDER = False\n",
    "    config.SIMULATOR.AGENT_0.SENSORS.append(\"THIRD_RGB_SENSOR\")\n",
    "    config.freeze()\n",
    "    return config\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "# If the import block fails due to an error like \"'PIL.TiffTags' has no attribute\n",
    "# 'IFD'\", then restart the Colab runtime instance and rerun this cell and the previous cell.\n",
    "import PIL\n",
    "\n",
    "importlib.reload(PIL.TiffTags)  # To potentially avoid PIL problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local installation\n",
    "\n",
    "For Habitat 2.0 functionality, install the `main` branch of Habitat Lab. Complete installation steps:\n",
    "\n",
    "1. Install [Habitat Sim](https://github.com/facebookresearch/habitat-sim#recommended-conda-packages) **using the `withbullet` option**. Linux example: `conda install habitat-sim withbullet headless -c conda-forge -c aihabitat-nightly`. MacOS example (does not include headless): `conda install habitat-sim withbullet -c conda-forge -c aihabitat-nightly`. Habitat Sim is not supported by Windows.\n",
    "2. Download the `main` branch of Habitat Lab: `git clone https://github.com/facebookresearch/habitat-lab.git`\n",
    "3. Install Habitat Lab: `cd habitat-lab && pip install -r requirements.txt && python setup.py develop --all`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "Start with a minimal environment interaction loop using the Habitat API. This sets up the environment, takes random episodes, and then saves a video once the episode ends.\n",
    "\n",
    "If this is your first time running Habitat 2.0 code, the datasets will automatically download which include the ReplicaCAD scenes, episode datasets, and object assets. To manually download this data, run `python -m habitat_sim.utils.datasets_download --uids rearrange_task_assets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with habitat.Env(\n",
    "    config=insert_render_options(\n",
    "        habitat.get_config(\n",
    "            os.path.join(\n",
    "                habitat_gym.base_dir,\n",
    "                \"configs/tasks/rearrange/pick.yaml\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ") as env:\n",
    "    observations = env.reset()  # noqa: F841\n",
    "\n",
    "    print(\"Agent acting inside environment.\")\n",
    "    count_steps = 0\n",
    "    # To save the video\n",
    "    video_file_path = \"data/example_interact.mp4\"\n",
    "    video_writer = vut.get_fast_video_writer(video_file_path, fps=30)\n",
    "\n",
    "    while not env.episode_over:\n",
    "        observations = env.step(env.action_space.sample())  # noqa: F841\n",
    "        info = env.get_metrics()\n",
    "\n",
    "        render_obs = observations_to_image(observations, info)\n",
    "        render_obs = overlay_frame(render_obs, info)\n",
    "\n",
    "        video_writer.append_data(render_obs)\n",
    "\n",
    "        count_steps += 1\n",
    "    print(\"Episode finished after {} steps.\".format(count_steps))\n",
    "\n",
    "    video_writer.close()\n",
    "    if vut.is_notebook():\n",
    "        vut.display_video(video_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym API\n",
    "You can also use environments through the Gym API. For more information about how to use the Gym API and the supported tasks, see [this tutorial](https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/colabs/habitat2_gym_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HabitatRenderPick-v0\")\n",
    "\n",
    "video_file_path = \"data/example_interact.mp4\"\n",
    "video_writer = vut.get_fast_video_writer(video_file_path, fps=30)\n",
    "\n",
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    video_writer.append_data(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "video_writer.close()\n",
    "if vut.is_notebook():\n",
    "    vut.display_video(video_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Play Script\n",
    "On your local machine with a display connected, play the tasks using the keyboard to control the robot:\n",
    "```\n",
    "python examples/interactive_play.py --play-task --never-end\n",
    "```\n",
    "For more information about the interactive play script, see the\n",
    "[documentation string at the top of the file](https://github.com/facebookresearch/habitat-lab/blob/main/examples/interactive_play.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Habitat Baselines\n",
    "Start training policies with PPO using [Habitat Baselines](https://github.com/facebookresearch/habitat-lab/tree/main/habitat_baselines#baselines). As an example, start training a pick policy with:\n",
    "\n",
    "```\n",
    "python -u habitat_baselines/run.py --exp-config habitat_baselines/config/rearrange/ddppo_pick.yaml --run-type train\n",
    "```\n",
    "Find the [complete list of RL configurations here](https://github.com/facebookresearch/habitat-lab/tree/main/habitat_baselines/config/rearrange), any config starting with `ddppo` can be substituted.\n",
    "\n",
    "See [here](https://github.com/facebookresearch/habitat-lab/tree/main/habitat_baselines#baselines) for more information on how to run with Habitat Baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Assistant Benchmark (HAB) Tasks\n",
    "\n",
    "To run the HAB tasks, use any of the training configurations here: [here](https://github.com/facebookresearch/habitat-lab/tree/main/main/config/rearrange/hab). For example, to run monolithic RL training on the Tidy House task run:\n",
    "```\n",
    "python -u habitat_baselines/run.py --exp-config habitat_baselines/config/rearrange/hab/ddppo_tidy_house.yaml --run-type train\n",
    "```\n",
    "To run the TP-SRL baseline use the [`tp_srl`](https://github.com/facebookresearch/habitat-lab/tree/main/habitat_baselines/config/rearrange/hab/tp_srl.yaml`) config. You will first need trained models for each of the individual skills placed in `data/models/[skill_name].pt`. Then specify the name of the task to run. For example, to run TP-SRL on the `set_table` task, run the following:\n",
    "```\n",
    "python -u habitat_baselines/run.py --exp-config habitat_baselines/config/rearrange/hab/tp_srl.yaml --run-type train BASE_TASK_CONFIG_PATH configs/tasks/rearrange/set_table.yaml\n",
    "```\n",
    "[`tp_srl_oracle_nav`](https://github.com/facebookresearch/habitat-lab/tree/main/habitat_baselines/config/rearrange/hab/tp_srl_oracle_nav.yaml) is the TP-SRL method with oracle navigation.\n",
    "\n",
    "The HAB tasks can also be used from the Gym interface (tutorial [here](https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/colabs/habitat2_gym_tutorial.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining New Tasks\n",
    "\n",
    "We will define a task for the robot to navigate to and then pick up a target object in the environment. To support a new task we need:\n",
    "* A task of type `RearrangeTask` which implements the reset function.\n",
    "* Sensor definitions to populate the observation space.\n",
    "* Measurement definitions to define the reward, termination condition, and additional logging information.\n",
    "\n",
    "For other examples of task, sensor, and measurement definitions, [see here\n",
    "for existing tasks](https://github.com/facebookresearch/habitat-lab/tree/main/habitat/tasks/rearrange/sub_tasks). Tasks, sensors, and measurements are connected through a config file that defines the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@registry.register_task(name=\"RearrangeDemoNavPickTask-v0\")\n",
    "class NavPickTaskV1(RearrangeTask):\n",
    "    \"\"\"\n",
    "    Primarily this is used to implement the episode reset functionality.\n",
    "    Can also implement custom episode step functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self, episode):\n",
    "        self.target_object_index = np.random.randint(\n",
    "            0, self._sim.get_n_targets()\n",
    "        )\n",
    "        start_pos = self._sim.pathfinder.get_random_navigable_point()\n",
    "        self._sim.robot.base_pos = start_pos\n",
    "\n",
    "        # Put any reset logic here.\n",
    "        return super().reset(episode)\n",
    "\n",
    "\n",
    "@registry.register_sensor\n",
    "class TargetStartSensor(Sensor):\n",
    "    \"\"\"\n",
    "    Relative position from end effector to target object start position.\n",
    "    \"\"\"\n",
    "\n",
    "    cls_uuid: str = \"relative_object_to_end_effector\"\n",
    "\n",
    "    def __init__(self, sim, config, *args, task, **kwargs):\n",
    "        self._sim = sim\n",
    "        self._task = task\n",
    "        self._config = config\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _get_uuid(self, *args, **kwargs):\n",
    "        return TargetStartSensor.cls_uuid\n",
    "\n",
    "    def _get_sensor_type(self, *args, **kwargs):\n",
    "        return SensorTypes.TENSOR\n",
    "\n",
    "    def _get_observation_space(self, *args, **kwargs):\n",
    "        return spaces.Box(\n",
    "            shape=(3,),\n",
    "            low=np.finfo(np.float32).min,\n",
    "            high=np.finfo(np.float32).max,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def get_observation(self, observations, episode, *args, **kwargs):\n",
    "        global_T = self._sim.robot.ee_transform\n",
    "        T_inv = global_T.inverted()\n",
    "        start_pos = self._sim.get_target_objs_start()[\n",
    "            self._task.target_object_index\n",
    "        ]\n",
    "        relative_start_pos = T_inv.transform_point(start_pos)\n",
    "        return relative_start_pos\n",
    "\n",
    "\n",
    "@registry.register_measure\n",
    "class DistanceToTargetObject(Measure):\n",
    "    \"\"\"\n",
    "    Gets the Euclidean distance to the target object from the end-effector.\n",
    "    \"\"\"\n",
    "\n",
    "    cls_uuid: str = \"distance_to_object\"\n",
    "\n",
    "    def __init__(self, sim, config, *args, **kwargs):\n",
    "        self._sim = sim\n",
    "        self._config = config\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_uuid(*args, **kwargs):\n",
    "        return DistanceToTargetObject.cls_uuid\n",
    "\n",
    "    def reset_metric(self, *args, episode, **kwargs):\n",
    "        self.update_metric(*args, episode=episode, **kwargs)\n",
    "\n",
    "    def update_metric(self, *args, task, episode, **kwargs):\n",
    "        ee_pos = self._sim.robot.ee_transform.translation\n",
    "\n",
    "        idxs, _ = self._sim.get_targets()\n",
    "        scene_pos = self._sim.get_scene_pos()[idxs[task.target_object_index]]\n",
    "\n",
    "        # Metric information is stored in the `self._metric` variable.\n",
    "        self._metric = np.linalg.norm(scene_pos - ee_pos, ord=2, axis=-1)\n",
    "\n",
    "\n",
    "@registry.register_measure\n",
    "class NavPickReward(RearrangeReward):\n",
    "    \"\"\"\n",
    "    For every new task, you NEED to implement a reward function.\n",
    "    `RearrangeReward` automatically includes penalties for collisions into the reward function.\n",
    "    \"\"\"\n",
    "\n",
    "    cls_uuid: str = \"navpick_reward\"\n",
    "\n",
    "    def __init__(self, sim, config, *args, **kwargs):\n",
    "        self._sim = sim\n",
    "        self._config = config\n",
    "        super().__init__(sim=sim, config=config, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_uuid(*args, **kwargs):\n",
    "        return NavPickReward.cls_uuid\n",
    "\n",
    "    def reset_metric(self, *args, task, episode, **kwargs):\n",
    "        # Measurements can be computed from other measurements.\n",
    "        task.measurements.check_measure_dependencies(\n",
    "            self.uuid,\n",
    "            [\n",
    "                DistanceToTargetObject.cls_uuid,\n",
    "            ],\n",
    "        )\n",
    "        self.update_metric(*args, task=task, episode=episode, **kwargs)\n",
    "\n",
    "    def update_metric(self, *args, task, episode, **kwargs):\n",
    "        ee_to_object_distance = task.measurements.measures[\n",
    "            DistanceToTargetObject.cls_uuid\n",
    "        ].get_metric()\n",
    "\n",
    "        self._metric = -ee_to_object_distance\n",
    "\n",
    "\n",
    "@registry.register_measure\n",
    "class NavPickSuccess(Measure):\n",
    "    \"\"\"\n",
    "    For every new task, you NEED to implement a \"success\" condition.\n",
    "    \"\"\"\n",
    "\n",
    "    cls_uuid: str = \"navpick_success\"\n",
    "\n",
    "    def __init__(self, sim, config, *args, **kwargs):\n",
    "        self._sim = sim\n",
    "        self._config = config\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_uuid(*args, **kwargs):\n",
    "        return NavPickSuccess.cls_uuid\n",
    "\n",
    "    def reset_metric(self, *args, episode, task, observations, **kwargs):\n",
    "        self.update_metric(\n",
    "            *args,\n",
    "            episode=episode,\n",
    "            task=task,\n",
    "            observations=observations,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def update_metric(self, *args, episode, task, observations, **kwargs):\n",
    "        # Check that the agent is holding the correct object.\n",
    "        abs_targ_obj_idx = self._sim.scene_obj_ids[task.target_object_index]\n",
    "        self._metric = abs_targ_obj_idx == self._sim.grasp_mgr.snap_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add all the previously defined task, sensor, and measurement\n",
    "definitions to a config file to finish defining the new Habitat task. For\n",
    "examples of more configs [see here](https://github.com/facebookresearch/habitat-lab/tree/main/configs/tasks/rearrange).\n",
    "\n",
    "This config also defines the action space through the `TASK.ACTIONS` key. You\n",
    "can substitute different base control actions from\n",
    "[here](https://github.com/facebookresearch/habitat-lab/blob/main/habitat/tasks/rearrange/actions.py),\n",
    "different arm control actions [from\n",
    "here](https://github.com/facebookresearch/habitat-lab/blob/main/habitat/tasks/rearrange/actions.py),\n",
    "and different grip actions [from here](https://github.com/facebookresearch/habitat-lab/blob/main/habitat/tasks/rearrange/grip_actions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_txt = \"\"\"\n",
    "ENVIRONMENT:\n",
    "    # Number of steps within an episode.\n",
    "    MAX_EPISODE_STEPS: 200\n",
    "DATASET:\n",
    "    TYPE: RearrangeDataset-v0\n",
    "    SPLIT: train\n",
    "    # The dataset to use. Later we will generate our own dataset.\n",
    "    DATA_PATH: data/datasets/replica_cad/rearrange/v1/{split}/all_receptacles_10k_1k.json.gz\n",
    "    SCENES_DIR: \"data/replica_cad/\"\n",
    "TASK:\n",
    "    TYPE: RearrangeDemoNavPickTask-v0\n",
    "\n",
    "    # Sensors for the observation space.\n",
    "    TARGET_START_SENSOR:\n",
    "        TYPE: \"TargetStartSensor\"\n",
    "    JOINT_SENSOR:\n",
    "        TYPE: \"JointSensor\"\n",
    "        DIMENSIONALITY: 7\n",
    "    SENSORS: [\"TARGET_START_SENSOR\", \"JOINT_SENSOR\"]\n",
    "\n",
    "    # Measurements\n",
    "    ROBOT_FORCE:\n",
    "        TYPE: \"RobotForce\"\n",
    "        MIN_FORCE: 20.0\n",
    "    FORCE_TERMINATE:\n",
    "        TYPE: \"ForceTerminate\"\n",
    "        # Maximum amount of allowed force in Newtons.\n",
    "        MAX_ACCUM_FORCE: 5000.0\n",
    "    DISTANCE_TO_TARGET_OBJECT:\n",
    "        TYPE: \"DistanceToTargetObject\"\n",
    "    NAV_PICK_REWARD:\n",
    "        TYPE: \"NavPickReward\"\n",
    "        SCALING_FACTOR: 0.1\n",
    "\n",
    "        # General Rearrange Reward config\n",
    "        CONSTRAINT_VIOLATE_PEN: 10.0\n",
    "        FORCE_PEN: 0.001\n",
    "        MAX_FORCE_PEN: 1.0\n",
    "        FORCE_END_PEN: 10.0\n",
    "\n",
    "    NAV_PICK_SUCCESS:\n",
    "        TYPE: \"NavPickSuccess\"\n",
    "\n",
    "    MEASUREMENTS:\n",
    "        # The measurements returned in the info dictionary\n",
    "        - \"ROBOT_FORCE\"\n",
    "        - \"FORCE_TERMINATE\"\n",
    "        - \"DISTANCE_TO_TARGET_OBJECT\"\n",
    "        - \"NAV_PICK_REWARD\"\n",
    "        - \"NAV_PICK_SUCCESS\"\n",
    "    ACTIONS:\n",
    "        # Define the action space.\n",
    "        ARM_ACTION:\n",
    "            TYPE: \"ArmAction\"\n",
    "            ARM_CONTROLLER: \"ArmRelPosAction\"\n",
    "            GRIP_CONTROLLER: \"MagicGraspAction\"\n",
    "            ARM_JOINT_DIMENSIONALITY: 7\n",
    "            GRASP_THRESH_DIST: 0.15\n",
    "            DISABLE_GRIP: False\n",
    "            DELTA_POS_LIMIT: 0.0125\n",
    "            EE_CTRL_LIM: 0.015\n",
    "        BASE_VELOCITY:\n",
    "            TYPE: \"BaseVelAction\"\n",
    "            LIN_SPEED: 12.0\n",
    "            ANG_SPEED: 12.0\n",
    "            ALLOW_DYN_SLIDE: True\n",
    "            END_ON_STOP: False\n",
    "            ALLOW_BACK: True\n",
    "            MIN_ABS_LIN_SPEED: 1.0\n",
    "            MIN_ABS_ANG_SPEED: 1.0\n",
    "    POSSIBLE_ACTIONS:\n",
    "        - ARM_ACTION\n",
    "        - BASE_VELOCITY\n",
    "\n",
    "SIMULATOR:\n",
    "    ADDITIONAL_OBJECT_PATHS:\n",
    "        - \"data/objects/ycb/configs/\"\n",
    "    DEBUG_RENDER: False\n",
    "    ACTION_SPACE_CONFIG: v0\n",
    "    AGENTS: ['AGENT_0']\n",
    "    CONCUR_RENDER: False\n",
    "    AUTO_SLEEP: False\n",
    "    AGENT_0:\n",
    "        HEIGHT: 1.5\n",
    "        IS_SET_START_STATE: False\n",
    "        RADIUS: 0.1\n",
    "        SENSORS: ['HEAD_RGB_SENSOR']\n",
    "        START_POSITION: [0, 0, 0]\n",
    "        START_ROTATION: [0, 0, 0, 1]\n",
    "    HEAD_RGB_SENSOR:\n",
    "        WIDTH: 128\n",
    "        HEIGHT: 128\n",
    "\n",
    "    # Agent setup\n",
    "    ARM_REST: [0.6, 0.0, 0.9]\n",
    "    CTRL_FREQ: 120.0\n",
    "    AC_FREQ_RATIO: 4\n",
    "    ROBOT_URDF: ./data/robots/hab_fetch/robots/hab_fetch.urdf\n",
    "    ROBOT_TYPE: \"FetchRobot\"\n",
    "    FORWARD_STEP_SIZE: 0.25\n",
    "\n",
    "    # Grasping\n",
    "    HOLD_THRESH: 0.09\n",
    "    GRASP_IMPULSE: 1000.0\n",
    "\n",
    "    HABITAT_SIM_V0:\n",
    "        ALLOW_SLIDING: True\n",
    "        ENABLE_PHYSICS: True\n",
    "        GPU_DEVICE_ID: 0\n",
    "        GPU_GPU: False\n",
    "        PHYSICS_CONFIG_FILE: ./data/default.physics_config.json\n",
    "    TYPE: RearrangeSim-v0\n",
    "\"\"\"\n",
    "nav_pick_cfg_path = \"data/nav_pick_demo.yaml\"\n",
    "with open(nav_pick_cfg_path, \"w\") as f:\n",
    "    f.write(cfg_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new task can then be imported via the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with habitat.Env(\n",
    "    config=insert_render_options(habitat.get_config(nav_pick_cfg_path))\n",
    ") as env:\n",
    "    env.reset()\n",
    "\n",
    "    print(\"Agent acting inside environment.\")\n",
    "    count_steps = 0\n",
    "    # To save the video\n",
    "    video_file_path = \"data/example_interact.mp4\"\n",
    "    video_writer = vut.get_fast_video_writer(video_file_path, fps=30)\n",
    "\n",
    "    while not env.episode_over:\n",
    "        action = env.action_space.sample()\n",
    "        observations = env.step(action)  # noqa: F841\n",
    "        info = env.get_metrics()\n",
    "\n",
    "        render_obs = observations_to_image(observations, info)\n",
    "        render_obs = overlay_frame(render_obs, info)\n",
    "\n",
    "        video_writer.append_data(render_obs)\n",
    "\n",
    "        count_steps += 1\n",
    "    print(\"Episode finished after {} steps.\".format(count_steps))\n",
    "\n",
    "    video_writer.close()\n",
    "    if vut.is_notebook():\n",
    "        vut.display_video(video_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "The previously defined task uses an included default `all_receptacles_10k_1k.json.gz` dataset which places objects on any receptacle. The episode `.json.gz` dataset defines where\n",
    "objects are placed and their rearrangement target positions. New episode\n",
    "datasets are generated with the [rearrange_generator.py](https://github.com/facebookresearch/habitat-lab/blob/main/habitat/datasets/rearrange/rearrange_generator.py) script. In this example, we will define a new episode dataset where a single object spawns on the table with its goal also on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cfg_txt = \"\"\"\n",
    "---\n",
    "dataset_path: \"data/replica_cad/replicaCAD.scene_dataset_config.json\"\n",
    "additional_object_paths:\n",
    "  - \"data/objects/ycb/configs/\"\n",
    "scene_sets:\n",
    "  -\n",
    "    name: \"v3_sc\"\n",
    "    included_substrings:\n",
    "      - \"v3_sc\"\n",
    "    excluded_substrings: []\n",
    "    comment: \"This set (v3_sc) selects all 105 ReplicaCAD variations with static furniture.\"\n",
    "\n",
    "object_sets:\n",
    "  -\n",
    "    name: \"kitchen\"\n",
    "    included_substrings:\n",
    "      - \"002_master_chef_can\"\n",
    "      - \"003_cracker_box\"\n",
    "    excluded_substrings: []\n",
    "    comment: \"Leave included_substrings empty to select all objects.\"\n",
    "\n",
    "receptacle_sets:\n",
    "  -\n",
    "    name: \"table\"\n",
    "    included_object_substrings:\n",
    "      - \"frl_apartment_table_01\"\n",
    "    excluded_object_substrings: []\n",
    "    included_receptacle_substrings:\n",
    "      - \"\"\n",
    "    excluded_receptacle_substrings: []\n",
    "    comment: \"The empty substrings act like wildcards, selecting all receptacles for all objects.\"\n",
    "\n",
    "scene_sampler:\n",
    "  type: \"subset\"\n",
    "  params:\n",
    "    scene_sets: [\"v3_sc\"]\n",
    "  comment: \"Samples from ReplicaCAD 105 variations with static furniture.\"\n",
    "\n",
    "\n",
    "object_samplers:\n",
    "  -\n",
    "    name: \"kitchen_counter\"\n",
    "    type: \"uniform\"\n",
    "    params:\n",
    "      object_sets: [\"kitchen\"]\n",
    "      receptacle_sets: [\"table\"]\n",
    "      num_samples: [1, 1]\n",
    "      orientation_sampling: \"up\"\n",
    "\n",
    "object_target_samplers:\n",
    "  -\n",
    "    name: \"kitchen_counter_targets\"\n",
    "    type: \"uniform\"\n",
    "    params:\n",
    "      object_samplers: [\"kitchen_counter\"]\n",
    "      receptacle_sets: [\"table\"]\n",
    "      num_samples: [1, 1]\n",
    "      orientation_sampling: \"up\"\n",
    "\"\"\"\n",
    "nav_pick_cfg_path = \"data/nav_pick_dataset.yaml\"\n",
    "with open(nav_pick_cfg_path, \"w\") as f:\n",
    "    f.write(dataset_cfg_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m habitat.datasets.rearrange.rearrange_generator --run --config data/nav_pick_dataset.yaml --num-episodes 10 --out data/nav_pick.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this dataset set `DATASET.DATA_PATH = data/nav_pick.json.gz` in the task config. See the full set of possible objects, receptacles, and scenes with `python -m habitat.datasets.rearrange.rearrange_generator --list`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Habitat 2.0 Quick Start Tutorial",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "nb_python//py:percent,colabs//ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
